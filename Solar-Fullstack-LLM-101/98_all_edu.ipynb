{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suyeonlee01/insight-12th/blob/main/Solar-Fullstack-LLM-101/98_all_edu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyo-KcZ0QDiK"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/98_all_edu.ipynb\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go3V7u56QDiN"
      },
      "source": [
        "# Upstage Solar Full Stack LLM 101\n",
        "## Code to Understand!\n",
        "![Overview](https://github.com/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/figures/overview.png?raw=1)\n",
        "\n",
        "\n",
        "### Table of Contents\n",
        "* <b> Session 1. Hello Solar </b> : Obtain an Upstage API Key and use the upstage chat model. <br>\n",
        "    - 1-1 Interacting with the Solar-1-mini-chat Model\n",
        "    - 1-2 Using Few-Shot Examples in Chat Completions\n",
        "\n",
        "\n",
        "- <b> Session 2. Building LLM Applications with LangChain</b> :  Learn how to easily implement LLM chains using LangChain and understand the features of LLMs.<br>\n",
        "    - 2-1 Prompt Engineering\n",
        "    - 2-2 Hallucinations\n",
        "    - 2-3 Groundedness Check with LangChain and Upstage\n",
        "\n",
        "\n",
        "- <b> Session 3. What is RAG? </b>:  Understand the concept of RAG, load documents, and implement a RAG system.<br>\n",
        "    - 3-1 Layout Analysis\n",
        "    - 3-2 Retrieval Augmented Generation (RAG) for Question Answering\n",
        "    - 3-3 RAG Limitations <br>\n",
        "\n",
        "\n",
        "- <b> Session 4. Efficient Text Splitting and Indexing with LangChain </b>:  Efficiently build a RAG system by loading external documents, splitting them into smaller chunks, using embedding APIs to store them in a vectorspace, and retrieving them.<br>\n",
        "\n",
        "- <b> Session 5. Gradio </b>: Use Gradio and RAG techniques to process PDF documents and generate real-time, interactive responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzwKlORQQDiO",
        "outputId": "a16745d8-e1a0-45eb-d2f3-e7b605f86baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: typer 0.12.5 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.3/230.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.1/387.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.1/208.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.7/615.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.2/108.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.3/110.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clipboard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.0/306.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip3 install -qU guardrails-ai openai langchain_community langchain_experimental langchain-upstage sentence-transformers langchainhub langchain-chroma langchain matplotlib python-dotenv tavily-python ragas faiss-cpu tokenizers getpass4\n",
        "! pip3 install -q arize-phoenix[evals]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioJOdKhqQDiP"
      },
      "source": [
        "## [Session 1] HELLO SOLAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAJNZSTXQDiP"
      },
      "source": [
        "  \n",
        "<b> Introduction to Solar Framework </b>: Learn the basics of setting up the Solar LLM framework and running a simple \"Hello, World!\" example to understand its core functionality.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paEsragIQDiP"
      },
      "source": [
        "#### UPSTAGE_API_KEY\n",
        "To obtain your Upstage API key, follow these steps:\n",
        "\n",
        "1. Visit the Upstage AI console at <https://console.upstage.ai>.\n",
        "2. Sign up for an account if you don't already have one.\n",
        "3. Log in to your account.\n",
        "4. Navigate to the API key section.\n",
        "5. Generate your API key.\n",
        "6. Copy the key and save it securely.\n",
        "\n",
        "![Console](https://github.com/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/figures/console.upstage.ai.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIdPsoQrQDiQ",
        "outputId": "14896422-6cdf-497f-8450-091b482eb271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cannot find .env file\n"
          ]
        }
      ],
      "source": [
        "%load_ext dotenv\n",
        "%dotenv\n",
        "# UPSTAGE_API_KEY from https://console.upstage.ai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AK1xJTqQDiQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 여기까지는 보안 관련 코드인 것 같습니다."
      ],
      "metadata": {
        "id": "_RRVM2GmvlhE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ7wDaYOQDiQ"
      },
      "outputs": [],
      "source": [
        "# @title set API key\n",
        "import os\n",
        "import getpass\n",
        "from pprint import pprint\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from IPython import get_ipython\n",
        "\n",
        "if \"google.colab\" in str(get_ipython()):\n",
        "    # Running in Google Colab. Please set the UPSTAGE_API_KEY in the Colab Secrets\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")\n",
        "else:\n",
        "    # Running locally. Please set the UPSTAGE_API_KEY in the .env file\n",
        "    from dotenv import load_dotenv\n",
        "\n",
        "    load_dotenv()\n",
        "\n",
        "if \"UPSTAGE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"UPSTAGE_API_KEY\"] = getpass.getpass(\"Enter your Upstage API key: \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Colab과 로컬 환경 모두에서 Upstage API 키를 설정하는 방법을 제공합니다."
      ],
      "metadata": {
        "id": "ON2qckSPv3iu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mHHONwJQDiQ"
      },
      "source": [
        "####  1-1 Interacting with the Solar-1-mini-chat Model\n",
        "\n",
        "This Python code demonstrates how to use the OpenAI API to interact with the Solar-1-mini-chat model provided by Upstage AI.\n",
        "\n",
        "##### Steps\n",
        "\n",
        "1. Import necessary libraries: `os`, `openai`, and `pprint`.\n",
        "2. Set up the OpenAI client with the API key and base URL.\n",
        "3. Create a chat completion request using `client.chat.completions.create()`.\n",
        "   - Specify the model: \"solar-1-mini-chat\".\n",
        "   - Provide a list of messages, including the system message and user message.\n",
        "4. Handle the model's response:\n",
        "   - Print the entire response using `pprint()`.\n",
        "   - Print the content of the assistant's message using `response.choices[0].message.content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXOoVFwPQDiR",
        "outputId": "4b918caf-cb82-41fa-f081-20720dba5c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='376fc318-556f-4ea4-bd2d-e6f1546f7712', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Oh, Korea! That's a beautiful place with a rich history and culture. I haven't been there yet, but I'd love to visit someday. The food is supposed to be amazing, and I'm curious about trying Kimchi, which is a traditional fermented vegetable dish. I'd also love to see the Demilitarized Zone (DMZ) and learn more about the history of the Korean War. And, of course, I'd love to meet new friends and share stories with them.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730507327, model='solar-mini-240612', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=113, prompt_tokens=26, total_tokens=139, completion_tokens_details=None, prompt_tokens_details=None))\n",
            "Message only:\n",
            "(\"Oh, Korea! That's a beautiful place with a rich history and culture. I \"\n",
            " \"haven't been there yet, but I'd love to visit someday. The food is supposed \"\n",
            " \"to be amazing, and I'm curious about trying Kimchi, which is a traditional \"\n",
            " \"fermented vegetable dish. I'd also love to see the Demilitarized Zone (DMZ) \"\n",
            " \"and learn more about the history of the Korean War. And, of course, I'd love \"\n",
            " 'to meet new friends and share stories with them.')\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from pprint import pprint\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ[\"UPSTAGE_API_KEY\"], base_url=\"https://api.upstage.ai/v1/solar\"\n",
        ")\n",
        "chat_result = client.chat.completions.create(\n",
        "    model=\"solar-1-mini-chat\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What about Korea?\"},\n",
        "    ],\n",
        ")\n",
        "pprint(chat_result)\n",
        "print(\"Message only:\")\n",
        "pprint(chat_result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upstage의 Solar 모델을 활용하여 AI와 대화를 나누는 예시입니다."
      ],
      "metadata": {
        "id": "vdnHDQsrwa05"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-IBauz3QDiR"
      },
      "source": [
        "#### 1-2 Using Few-Shot Examples in Chat Completions\n",
        "\n",
        "This Python code demonstrates how to use few-shot examples in the OpenAI Chat Completions API to provide context and guide the model's responses.\n",
        "\n",
        "##### Steps\n",
        "\n",
        "1. Set up the OpenAI client with the API key and base URL.\n",
        "2. Create a chat completion request using `client.chat.completions.create()`.\n",
        "   - Specify the model: \"solar-1-mini-chat\".\n",
        "   - Provide a list of messages, including:\n",
        "     - System message: Defines the assistant's role.\n",
        "     - Few-shot examples: Provide context and desired behavior.\n",
        "     - User input: The actual user query.\n",
        "3. Handle the model's response:\n",
        "   - Print the entire response using `pprint()`.\n",
        "   - Print the content of the assistant's message using `response.choices[0].message.content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghf8mi3nQDiR",
        "outputId": "6609efcb-93dc-49fa-814f-3533825c0feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='46379a7c-b3ee-48a9-962d-3b1be1456bec', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm not sure, but I'll find out.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730507329, model='solar-mini-240612', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=14, prompt_tokens=55, total_tokens=69, completion_tokens_details=None, prompt_tokens_details=None))\n",
            "Message only:\n",
            "\"I'm not sure, but I'll find out.\"\n"
          ]
        }
      ],
      "source": [
        "# few shots: examples or history\n",
        "chat_result = client.chat.completions.create(\n",
        "    model=\"solar-1-mini-chat\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        # examples\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"I know of it. It's Paris!!\",\n",
        "        },\n",
        "        # user input\n",
        "        {\"role\": \"user\", \"content\": \"What about Korea?\"},\n",
        "    ],\n",
        ")\n",
        "pprint(chat_result)\n",
        "print(\"Message only:\")\n",
        "pprint(chat_result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few-Shot Learning 관련 설명:\n",
        "Few-shot learning은 소량의 학습 데이터만으로도 AI 모델이 새로운 작업을 수행할 수 있도록 하는 머신러닝 기법입니다. 이 기술은 특히 데이터가 제한된 상황에서 유용하며, 적은 양의 예시로도 모델이 일반화 능력을 발휘할 수 있도록 훈련됩니다.\n",
        "Few-shot learning의 핵심은 모델이 몇 개의 예시만 보고도 새로운 데이터에 대해 잘 예측할 수 있도록 하는 것입니다."
      ],
      "metadata": {
        "id": "clEZo3kpxIs_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEjnWz58QDiS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W3d9A-JQDiS"
      },
      "source": [
        "## [Session 2] Building LLM Applications with LangChain\n",
        "\n",
        "This Python code demonstrates how to use the LangChain library to build applications with Large Language Models (LLMs). It covers the basic steps of defining an LLM, creating a chat prompt, defining a chain, and invoking the chain.\n",
        "\n",
        "#### Steps\n",
        "\n",
        "1. Define your favorite LLM:\n",
        "   - Import the `ChatUpstage` class from `langchain_upstage`.\n",
        "   - Create an instance of `ChatUpstage` and assign it to the variable `llm`.\n",
        "\n",
        "2. Define a chat prompt:\n",
        "   - Import the `ChatPromptTemplate` class from `langchain_core.prompts`.\n",
        "   - Create a `ChatPromptTemplate` instance using the `from_messages()` method.\n",
        "   - Provide a list of messages, including system messages, example conversations, and user input.\n",
        "\n",
        "3. Define a chain:\n",
        "   - Import the `StrOutputParser` class from `langchain_core.output_parsers`.\n",
        "   - Create a chain by combining the `rag_with_history_prompt`, `llm`, and `StrOutputParser()` using the pipe (`|`) operator.\n",
        "\n",
        "4. Invoke the chain:\n",
        "   - Call the `invoke()` method on the `chain` object, passing an empty dictionary (`{}`) as the input.\n",
        "   - Print the response obtained from the chain."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain 라이브러리를 사용하여 대형 언어 모델(LLM) 기반 애플리케이션을 구축하는 방법을 보여줍니다. 이 코드는 LLM을 정의하고, 대화 프롬프트를 생성하며, 체인을 정의하고, 체인을 호출하는 기본 단계를 다룹니다.\n",
        "\n",
        "선호하는 LLM 정의:\n",
        "langchain_upstage에서 ChatUpstage 클래스를 가져옵니다.\n",
        "ChatUpstage의 인스턴스를 생성하고 이를 변수 llm에 할당합니다.\n",
        "\n",
        "대화 프롬프트 정의:\n",
        "langchain_core.prompts에서 ChatPromptTemplate 클래스를 가져옵니다.\n",
        "from_messages() 메서드를 사용하여 ChatPromptTemplate 인스턴스를 생성합니다. 시스템 메시지, 예시 대화, 사용자 입력 등의 메시지 목록을 제공합니다.\n",
        "\n",
        "체인 정의:\n",
        "langchain_core.output_parsers에서 StrOutputParser 클래스를 가져옵니다.\n",
        "rag_with_history_prompt, llm, 그리고 StrOutputParser()를 파이프(|) 연산자를 사용하여 결합하여 체인을 생성합니다.\n",
        "\n",
        "체인 호출:\n",
        "체인 객체의 invoke() 메서드를 호출하여, 빈 딕셔너리({})를 입력으로 전달합니다.\n",
        "체인에서 얻은 응답을 출력합니다."
      ],
      "metadata": {
        "id": "AmlaJAsnxogz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain은 대규모 언어 모델(LLM)을 활용한 애플리케이션을 쉽게 구축할 수 있도록 도와주는 오픈 소스 프레임워크입니다. LLM은 GPT-4와 같은 사전 훈련된 AI 모델로, 자연어 처리(NLP) 작업을 수행하는 데 사용됩니다."
      ],
      "metadata": {
        "id": "sCRvuHsFziJ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPyNDG5aQDiS",
        "outputId": "2cd3cf76-76fa-4789-e7fd-7b750663968b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings or emotions. I'm here to assist you with any questions or tasks you have. How can I help you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 16, 'total_tokens': 58, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'solar-mini-240612', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b3bf598d-7cc8-4ade-8016-5f82a64acd47-0', usage_metadata={'input_tokens': 16, 'output_tokens': 42, 'total_tokens': 58, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Quick hello world\n",
        "from langchain_upstage import ChatUpstage\n",
        "\n",
        "llm = ChatUpstage()\n",
        "llm.invoke(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain을 사용하여 대화형 AI 시스템을 구축하는 과정"
      ],
      "metadata": {
        "id": "bpLOb1Oo0EOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatPromptTemplate을 사용하여 대화 히스토리를 포함한 프롬프트를 정의합니다. 이 프롬프트는 시스템 메시지, 이전 대화 내용, 그리고 현재 질문을 포함하고 있어 모델이 컨텍스트를 이해하고 적절한 응답을 생성"
      ],
      "metadata": {
        "id": "SqqwAY4k0GjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "대화 히스토리를 포함한 RAG(Retrieval-Augmented Generation) 시스템의 기본 구조를 보여줍니다. 이 시스템은 이전 대화 내용을 고려하여 한국의 수도에 대한 질문에 답변"
      ],
      "metadata": {
        "id": "aVXHNOec1IXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "체인 (Chain)\n",
        "체인은 여러 작업이나 프로세스를 순차적으로 연결한 것을 의미합니다5.\n",
        "LangChain에서 체인은 여러 단계의 작업을 순서대로 실행하는 프로세스를 말합니다.\n",
        "각 단계는 이전 단계의 출력을 입력으로 받아 처리합니다.\n",
        "복잡한 작업을 작은 단계들로 나누어 처리할 수 있게 해줍니다.\n",
        "쉽게 말해, 체인은 여러 작업을 연결해서 하나의 큰 작업을 수행하는 방식이라고 볼 수 있습니다.\n",
        "\n",
        "쿼리 (Query)\n",
        "쿼리는 데이터베이스에 정보를 요청하는 것을 의미합니다26.\n",
        "데이터베이스에서 원하는 정보를 검색하기 위한 요청입니다.\n",
        "예를 들어, \"프랑스의 수도는 무엇인가요?\"라는 질문이 하나의 쿼리가 될 수 있습니다.\n",
        "데이터베이스는 이 쿼리를 처리하여 관련된 정보(이 경우 \"파리\")를 반환합니다.\n",
        "간단히 말해, 쿼리는 데이터베이스에 무언가를 물어보는 것과 같습니다.\n",
        "\n",
        "파싱 (Parsing)\n",
        "파싱은 데이터를 분석하고 의미 있는 형태로 변환하는 과정입니다1.\n",
        "구조화되지 않은 데이터(예: 문자열)를 분석하여 구조화된 형태로 변환합니다.\n",
        "예를 들어, \"12 * 5 - 5 / 3\"이라는 수식을 파싱하면 각 숫자와 연산자를 구분하여 처리할 수 있는 형태로 만듭니다.\n",
        "프로그래밍에서는 코드를 읽고 해석하는 과정도 파싱이라고 할 수 있습니다."
      ],
      "metadata": {
        "id": "V18Gxb_Q0TJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2zxni8AQDiS",
        "outputId": "09a971ab-f7d9-434b-be01-b16c92a71012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, Korea is a bit tricky. There are two Koreas, North Korea and South Korea. The capital of South Korea is Seoul, while the capital of North Korea is Pyongyang.\n"
          ]
        }
      ],
      "source": [
        "# langchain, 1. llm define, 2. prompt define, 3. chain, 4. chain.invoke\n",
        "\n",
        "# 1. define your favorate llm, solar\n",
        "from langchain_upstage import ChatUpstage\n",
        "\n",
        "llm = ChatUpstage()\n",
        "\n",
        "# 2. define chat prompt\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"What is the capital of France?\"),\n",
        "        (\"ai\", \"I know of it. It's Paris!!\"),\n",
        "        (\"human\", \"What about Korea?\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. define chain\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 4. invoke the chain\n",
        "c_result = chain.invoke({})\n",
        "print(c_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqyegTQIQDiS"
      },
      "source": [
        "### 2-1 Prompt Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_D17sQ7QDiS"
      },
      "source": [
        "#### Parameterized Prompt Templates in LangChain\n",
        "\n",
        "##### Overview\n",
        "\n",
        "- Prompt templates allow for reusable and modular prompts\n",
        "- They improve maintainability compared to using raw prompt strings\n",
        "- {country} value can be set from outside"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Engineering\n",
        "Prompt Engineering은 언어 모델에 최적의 입력을 제공하여 원하는 출력을 얻는 기술입니다.\n",
        "이는 다음과 같은 요소들을 포함합니다:\n",
        "\n",
        "명확하고 구체적인 지시사항 제공\n",
        "적절한 컨텍스트 설정\n",
        "예시 포함 (few-shot learning)\n",
        "출력 형식 지정\n",
        "제약 조건 설정\n",
        "효과적인 프롬프트 엔지니어링을 통해 모델의 성능을 크게 향상시킬 수 있습니다.\n",
        "\n",
        "LangChain의 매개변수화된 프롬프트 템플릿\n",
        "LangChain은 재사용 가능하고 모듈화된 프롬프트를 만들기 위해 PromptTemplate을 제공합니다. 주요 특징은 다음과 같습니다:\n",
        "재사용성: 프롬프트 구조를 한 번 정의하고 여러 곳에서 재사용할 수 있습니다.\n",
        "모듈성: 프롬프트 로직을 비즈니스 로직과 분리하여 코드의 가독성을 높입니다.\n",
        "동적 생성: 런타임에 사용자 입력이나 다른 요소에 따라 프롬프트를 동적으로 생성할 수 있습니다.\n",
        "유지보수성: 프롬프트 구조를 변경할 때 한 곳만 수정하면 되므로 유지보수가 용이합니다."
      ],
      "metadata": {
        "id": "epT3YSGB2IW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnQHcIcHQDiS",
        "outputId": "a9627538-8cbf-4fd2-b38e-bae42b60e267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, Korea! You see, Korea is divided into two countries: North Korea and South Korea. The capital of South Korea, which is the one most people think of when they hear \"Korea,\" is Seoul. It's a bustling city with a rich history and a lot of modern technology. North Korea's capital is Pyongyang, but information about it is harder to come by.\n",
            "---\n",
            "Oh, I'm not sure about Japan. Let me think... It's Tokyo, right?\n"
          ]
        }
      ],
      "source": [
        "# parameterized prompt template\n",
        "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"What is the capital of France?\"),\n",
        "        (\"ai\", \"I know of it. It's Paris!!\"),\n",
        "        (\"human\", \"What about {country}?\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 4. invoke chain with param\n",
        "print(chain.invoke({\"country\": \"Korea\"}))\n",
        "print(\"---\")\n",
        "print(chain.invoke({\"country\": \"Japan\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "대화 히스토리와 함께 {country} 매개변수를 포함합니다. 이를 통해 다양한 국가에 대해 동일한 질문"
      ],
      "metadata": {
        "id": "RZq7-ltj2wWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chain.invoke() 메서드를 사용하여 체인을 실행합니다. 매개변수로 {\"country\": \"Korea\"}와 {\"country\": \"Japan\"}을 전달하여 각각 한국과 일본의 수도에 대해 질문합니다."
      ],
      "metadata": {
        "id": "EugFeivI3TEz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlXM71V6QDiS"
      },
      "source": [
        "#### Leveraging Message History in LangChain Prompts\n",
        "\n",
        "- LangChain provides powerful tools for managing conversation history\n",
        "- `MessagesPlaceholder` allows for dynamic inclusion of message history\n",
        "- `HumanMessage` and `AIMessage` classes represent individual messages\n",
        "- Combining message history with user input enables context-aware responses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leveraging Message History in LangChain Prompts는 LangChain에서 대화의 문맥을 유지하고 연속적인 응답을 생성하는 기능입니다. LangChain은 이전 메시지와 사용자의 입력을 결합하여 문맥을 반영한 응답을 생성할 수 있도록 여러 도구를 제공합니다.\n",
        "\n",
        "MessagesPlaceholder:\n",
        "MessagesPlaceholder는 대화 기록을 동적으로 포함할 수 있게 해주는 요소입니다. 이를 통해, 대화의 흐름을 반영하여 더 자연스럽고 문맥에 맞는 응답을 생성할 수 있습니다.\n",
        "MessagesPlaceholder는 프롬프트 템플릿에 사용되어, 이전 대화 내용이 자동으로 입력되고 새 메시지를 추가할 때마다 업데이트됩니다.\n",
        "HumanMessage와 AIMessage:\n",
        "\n",
        "HumanMessage: 사용자가 입력한 메시지를 나타내는 클래스입니다. 사용자와의 대화에서 입력된 텍스트는 HumanMessage로 표현됩니다.\n",
        "AIMessage: AI 모델이 생성한 응답을 나타내는 클래스입니다. 모델이 제공한 답변을 AIMessage로 기록합니다.\n",
        "이러한 클래스를 사용하면 대화 내 메시지의 출처를 명확히 구분할 수 있어, 후속 질문에 대해 문맥을 더 정확하게 파악할 수 있습니다.\n",
        "문맥을 반영한 응답 생성:\n",
        "\n",
        "MessagesPlaceholder를 포함한 메시지 히스토리를 사용하여, 사용자가 후속 질문을 했을 때 이전 대화 내용을 참조하여 답변을 생성합니다.\n",
        "예를 들어, LangChain에서 이전 대화의 흐름을 반영해 사용자가 구체적인 맥락 없이 \"그게 언제였지?\"와 같은 질문을 할 경우, 히스토리를 통해 더 정확하게 대답할 수 있습니다."
      ],
      "metadata": {
        "id": "yDjRuygTp8-J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNUvxstcQDiT",
        "outputId": "36631cc9-0edc-404d-c73f-4f86547d86d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, for Korea, it's Seoul!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# More general chat\n",
        "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "history = [\n",
        "    HumanMessage(\"What is the capital of France?\"),\n",
        "    AIMessage(\"It's Paris!!\"),\n",
        "]\n",
        "\n",
        "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
        "chain_result = chain.invoke({\"history\": history, \"input\": \"What about Korea?\"})\n",
        "print(chain_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HumanMessage는 사용자의 질문을 나타내고, AIMessage는 AI의 응답을 나타냅니다."
      ],
      "metadata": {
        "id": "axK6jtEK4KA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chain.invoke() 메서드를 호출하여 체인을 실행합니다.\n",
        "매개변수로 {\"history\": history, \"input\": \"What about Korea?\"}를 전달합니다. 여기서 history는 이전 대화 내용을 포함하고, input은 새로운 질문(한국의 수도에 대한 질문)을 나타냅니다.\n",
        "결과는 chain_result에 저장되고, 이를 출력합니다."
      ],
      "metadata": {
        "id": "_7P-RZx44Hsp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62XIsXodQDiT"
      },
      "source": [
        "#### Chain of Thought Prompting\n",
        "\n",
        "![CoT](https://github.com/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/figures/cot.webp?raw=1)\n",
        "\n",
        "from https://arxiv.org/abs/2201.11903"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1UNp1_djQDiT",
        "outputId": "eb079b3c-dd54-49d7-ddc7-fd9c3046b9e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The cafeteria started with 23 apples. They used 20 apples for lunch, so they had 23 - 20 = 3 apples left. Then they bought 6 more apples, so they had 3 + 6 = 9 apples.\\n\\nThe final answer is:\\n\\\\boxed{9}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Q: The cafeteria had 23 apples.\n",
        "If they used 20 to make lunch and bought 6 more,\n",
        "how many apples do they have?\n",
        "\n",
        "A: the answer is\n",
        "\"\"\"\n",
        ")\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "chain.invoke({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SW7flgUaQDiT",
        "outputId": "28115fd1-4311-4b66-c199-0f2a7e3fc4f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The cafeteria started with 23 apples. They used 20, so they had 3 left. Then they bought 6 more, so they had 9. The answer is 9.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
        "\n",
        "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
        "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
        "\n",
        "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"\"\"\n",
        ")\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "chain.invoke({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GywrPMzQDiT"
      },
      "source": [
        "#### Learn more advanced techniques by reading blog posts on Prompt Engineering!\n",
        "\n",
        "\n",
        "1. [[Prompt Engineering - Part 1] Maximizing the Use of LLM with Prompt Design](https://www.upstage.ai/feed/insight/prompt-engineering-guide-maximizing-the-use-of-llm-with-prompt-design)\n",
        "2. [[Prompt Engineering - Part 2] The Essence of Prompt Engineering: A Comprehensive Guide to Maximizing LLM Usage](https://www.upstage.ai/feed/insight/prompt-engineering-guide-to-maximizing-llm-usage)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prompt engineering 입력 방식"
      ],
      "metadata": {
        "id": "iEBoV0xsStsC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DysdrWYDQDiT"
      },
      "source": [
        "### 2-2 Hallucinations\n",
        "\n",
        "<b> Understanding Model Hallucinations </b> : Discover how to identify, understand, and mitigate hallucinations to ensure accurate and reliable model outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_HJ2ZM5QDiT"
      },
      "source": [
        "![Hallucination](https://github.com/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/figures/hallucination.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSJPthn0QDiU",
        "outputId": "25c5aa6f-0930-45c6-d7ce-c897e0a5a9d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The Upstage DUS (Dynamic User-centered System for Theatre) technique is a methodology developed by the Upstage Theatre Company to create interactive and immersive theatrical experiences. It combines elements of game design, user experience design, and storytelling to engage audience members in a dynamic and collaborative way.\\n\\nThe DUS system is designed to allow audience members to influence the course of the story through their choices and actions. This is achieved through a combination of technology, such as sensors and motion-capture systems, and traditional theatrical techniques, such as live actors and set design.\\n\\nThe Upstage DUS technique is used to create an environment where the audience is actively involved in the story, rather than being passive observers. This creates a more engaging and memorable experience for the audience, as they feel more connected to the characters and the story.\\n\\nOverall, the Upstage DUS technique is a powerful tool for creating immersive and interactive theatrical experiences that engage and inspire audiences.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 213, 'prompt_tokens': 18, 'total_tokens': 231, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'solar-mini-240612', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-18694bcb-ebbc-4293-9700-2a1454928ff6-0', usage_metadata={'input_tokens': 18, 'output_tokens': 213, 'total_tokens': 231, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Cannot say \"I don't know\" :-)\n",
        "# Because it is trained to complete the sentence and try to answer the question\n",
        "llm.invoke(\"What is Upstage DUS technique?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 환각이다..."
      ],
      "metadata": {
        "id": "Jokp0wk8Te9E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP1tlXUdQDiU"
      },
      "source": [
        "#### Next Token Prediction\n",
        "They are designed to generate the next words. It's also very difficult to know what we don't know.\n",
        "\n",
        "![image](https://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif)\n",
        "\n",
        "Image from https://jalammar.github.io/illustrated-gpt2/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Token Prediction은 언어 모델이 입력에 따라 가장 가능성이 높은 다음 단어나 토큰을 예측하여 문장을 생성하는 방식입니다. 하지만 모르는 정보를 모르는 채로 자신감 있게 예측하는 한계가 있으며, 이를 개선하기 위해 추가적인 사실 검증이나 프롬프트 최적화가 필요할 수 있습니다."
      ],
      "metadata": {
        "id": "lAO2af6nqyOS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KxVIuNQQDiU"
      },
      "source": [
        "### How Can We Mitigate Hallucinations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS2C24ISQDiU"
      },
      "source": [
        "### 2-3 Groundedness Check with LangChain and Upstage\n",
        "![Groundedness](https://github.com/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/figures/gc.png?raw=1)\n",
        "\n",
        "[Groundedness Check](https://developers.upstage.ai/docs/apis/groundedness-check)\n",
        "\n",
        "#### High-Level Overview\n",
        "\n",
        "The provided code demonstrates how to perform a groundedness check using the LangChain library and the Upstage model. The groundedness check is a process of verifying whether the generated response is grounded in the given context. This is an important step in ensuring the quality and relevance of the generated output.\n",
        "\n",
        "The code uses the `UpstageGroundednessCheck` class from the `langchain_upstage` module to perform the groundedness check. It takes the context (a string of unique documents) and the generated response as input, and returns a verdict indicating whether the response is grounded or not.\n",
        "\n",
        "#### Detailed Explanation\n",
        "\n",
        "1. The code starts by importing the necessary module:\n",
        "   - `UpstageGroundednessCheck` from `langchain_upstage`: This class is used to perform the groundedness check.\n",
        "   \n",
        "\n",
        "2. An instance of the `UpstageGroundednessCheck` class is created and assigned to the variable `groundedness_check`.\n",
        "\n",
        "3. The input for the groundedness check is prepared by creating a dictionary called `request_input`:\n",
        "   - The `\"context\"` key is assigned the value of `str(unique_docs)`, which represents the unique documents as a string.\n",
        "   - The `\"answer\"` key is assigned the value of `response`, which represents the generated response.\n",
        "   \n",
        "\n",
        "4. The `invoke` method of the `groundedness_check` instance is called with the `request_input` as an argument. This method performs the groundedness check and returns the verdict.\n",
        "\n",
        "5. The verdict is stored in the `gc_result` variable and printed to the console using `print(gc_result)`.\n",
        "\n",
        "6. The code then checks if the `gc_result` starts with the word \"grounded\" (case-insensitive):\n",
        "   - If the response starts with \"grounded\", it means the groundedness check has passed, and the message \"✅ Groundedness check passed\" is printed.\n",
        "   - If the response does not start with \"grounded\", it means the groundedness check has failed, and the message \"❌ Groundedness check failed\" is printed.\n",
        "\n",
        "\n",
        "The provided code demonstrates a simple yet effective way to perform a groundedness check using LangChain and Upstage. By verifying whether the generated response is grounded in the given context, it helps ensure the quality and relevance of the output.\n",
        "\n",
        "Groundedness checks are an important step in building reliable and trustworthy language models and conversational agents. They help prevent the generation of irrelevant, inconsistent, or factually incorrect responses.\n",
        "\n",
        "By using the `UpstageGroundednessCheck` class from LangChain, developers can easily integrate groundedness checks into their language model pipelines and improve the overall performance of their systems."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "주요 특징\n",
        "간편한 사용: LangChain의 UpstageGroundednessCheck 클래스를 사용하여 쉽게 groundedness 체크를 구현할 수 있습니다.\n",
        "유연성: 다양한 컨텍스트와 응답에 대해 체크를 수행할 수 있습니다.\n",
        "결과 해석: 체크 결과를 바로 해석하여 응답의 품질을 판단할 수 있습니다.\n",
        "이 코드는 언어 모델 파이프라인에 groundedness 체크를 쉽게 통합하여 시스템의 전반적인 성능을 향상시키는 데 도움을 줍니다. 이는 관련성 없는, 일관성 없는, 또는 사실적으로 부정확한 응답의 생성을 방지하는 데 중요한 역할을 합니다."
      ],
      "metadata": {
        "id": "vcNglUD0Tzdp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwEoeHSAQDiU",
        "outputId": "e240a240-6a10-4c68-8529-f4a0d3458cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grounded\n",
            "✅ Groundedness check passed\n"
          ]
        }
      ],
      "source": [
        "# GC\n",
        "from langchain_upstage import UpstageGroundednessCheck\n",
        "\n",
        "groundedness_check = UpstageGroundednessCheck()\n",
        "\n",
        "context = \"DUS is a new approach developed by Upstage to improve the search quality.\"\n",
        "answer = \"DUS is developed by Upstage.\"\n",
        "\n",
        "request_input = {\n",
        "    \"context\": context,\n",
        "    \"answer\": answer,\n",
        "}\n",
        "gc_result = groundedness_check.invoke(request_input)\n",
        "\n",
        "print(gc_result)\n",
        "if gc_result.lower().startswith(\"grounded\"):\n",
        "    print(\"✅ Groundedness check passed\")\n",
        "else:\n",
        "    print(\"❌ Groundedness check failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccLiedZtQDiX",
        "outputId": "488d5865-d45f-4ec3-b63c-982ad370af59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Groundedness check failed\n"
          ]
        }
      ],
      "source": [
        "context = \"DUS is a new approach developed by Upstage to improve the search quality.\"\n",
        "answer = \"DUS is developed by Google.\"\n",
        "\n",
        "request_input = {\n",
        "    \"context\": context,\n",
        "    \"answer\": answer,\n",
        "}\n",
        "gc_result = groundedness_check.invoke(request_input)\n",
        "\n",
        "if gc_result.lower().startswith(\"grounded\"):\n",
        "    print(\"✅ Groundedness check passed\")\n",
        "else:\n",
        "    print(\"❌ Groundedness check failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj-r89BdQDiY"
      },
      "source": [
        "## [Session 3] What is RAG?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ9gBX2QQDiY"
      },
      "source": [
        "Provide context and allow the language model to respond within that context only.\n",
        "\n",
        "![Overview](https://github.com/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/figures/rag.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZrdJ5X4QDiY"
      },
      "source": [
        "### 3-1 Layout Analysis\n",
        "\n",
        "Leveraging Layout Analyzer and LangChain for Efficient Text Splitting and Vectorization\n",
        "\n",
        "- Upstage Layout Analyzer extracts layouts, tables, and figures from any document\n",
        "- LangChain provides powerful tools for text splitting and vectorization\n",
        "\n",
        "![Layout Analyzer](https://github.com/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/figures/la.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upstage의 Layout Analyzer와 LangChain을 결합하여 효율적인 텍스트 분할 및 벡터화를 수행할 수 있습니다. 이 두 기술의 주요 특징과 장점은 다음과 같습니다:\n",
        "Upstage Layout Analyzer\n",
        "Upstage Layout Analyzer는 문서의 구조를 자동으로 이해하고 추출하는 강력한 도구입니다14:\n",
        "요소 검출: 헤더, 푸터, 단락, 캡션, 표, 이미지 등 다양한 문서 요소를 인식하고 추출합니다.\n",
        "문맥 기반 순서 정렬: 사람이 문서를 읽는 것처럼 문맥에 맞게 글자를 읽는 순서대로 데이터를 추출합니다.\n",
        "요소 간 관계 추출: 표와 캡션, 그림과 캡션 사이의 관계를 탐지하여 문서의 전체적인 맥락을 이해하기 쉽게 합니다.\n",
        "HTML 변환: 문서 구조를 인식한 결과를 HTML 코드로 변환할 수 있어 요소 단위로 수정이 가능합니다."
      ],
      "metadata": {
        "id": "DAY2K-kkVd7A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWA9BqLqQDiY"
      },
      "outputs": [],
      "source": [
        "from langchain_upstage import (\n",
        "    UpstageLayoutAnalysisLoader,\n",
        "    UpstageGroundednessCheck,\n",
        "    ChatUpstage,\n",
        "    UpstageEmbeddings,\n",
        ")\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "layzer = UpstageLayoutAnalysisLoader(\"/content/경제금융용어 700선_게시.pdf\", output_type=\"html\")\n",
        "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
        "docs = layzer.load()  # or layzer.lazy_load()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이아웃 분석: Upstage의 Layout Analyzer를 사용하여 PDF 문서의 구조를 자동으로 분석합니다.\n",
        "\n",
        "HTML 출력: 분석 결과를 HTML 형식으로 변환하여 문서 구조를 보존하고 쉽게 처리할 수 있게 합니다.\n",
        "\n",
        "유연한 로딩 옵션:\n",
        "load(): 전체 문서를 한 번에 메모리에 로드합니다. 작은 문서나 빠른 처리가 필요한 경우에 적합합니다.\n",
        "lazy_load(): 문서를 페이지 단위로 로드합니다. 대용량 문서를 처리할 때 메모리 사용을 최적화할 수 있습니다."
      ],
      "metadata": {
        "id": "YqjLonhaWBp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgDKl64ei0Ga",
        "outputId": "24de3583-8021-46b3-9dd5-77920b84bd58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    for doc in docs:\n",
        "        pprint(doc.page_content[:100])\n",
        "except NameError:\n",
        "    print(\"'docs' 변수가 정의되지 않았습니다. 먼저 문서를 로드해주세요.\")"
      ],
      "metadata": {
        "id": "hTGdqLvxXF4U",
        "outputId": "a67d6e9a-416e-4fb3-fd3e-901d30a760ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"<h1 id='2' style='font-size:22px'>Classifying Software Changes:<br>Clean or \"\n",
            " \"Buggy?</h1><br><p id='3'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXh5Wk_7QDiY",
        "outputId": "2b68912c-c1f0-4638-b2c1-7df793f0d7ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"<h1 id='2' style='font-size:22px'>Classifying Software Changes:<br>Clean or \"\n",
            " \"Buggy?</h1><br><p id='3'\")\n"
          ]
        }
      ],
      "source": [
        "for doc in docs:\n",
        "    pprint(doc.page_content[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "tCFihEUVQDic",
        "outputId": "67044336-ad03-488d-f9af-521694d511b0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1 id='2' style='font-size:22px'>Classifying Software Changes:<br>Clean or Buggy?</h1><br><p id='3' data-category='paragraph' style='font-size:20px'>Sunghun Kim, E. James Whitehead Jr., Member, IEEE, and Yi Zhang, Member, IEEE</p><p id='4' data-category='paragraph' style='font-size:16px'>Abstract-This paper introduces a new technique for predicting latent software bugs, called change classification. Change<br>classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes or<br>clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using<br>features (in the machine learning sense) extracted from the revision history of a software project stored in its software configuration<br>management repository. The trained classifier can classify changes as buggy or clean, with a 78 percent accuracy and a 60 percent<br>buggy change recall on average. Chan"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "display(HTML(docs[0].page_content[:1000]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KESYpDOQDid"
      },
      "source": [
        "### 3-2 Retrieval Augmented Generation (RAG) for Question Answering\n",
        "\n",
        "- RAG combines retrieval and generation to enhance LLM performance on specific tasks\n",
        "- Relevant context is retrieved from external data sources and added to the prompt\n",
        "- The augmented prompt is then passed to the LLM for generating a response\n",
        "- RAG is particularly useful for question answering on custom datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG는 정확하고 맥락에 맞는 답변을 생성하기 위해 다단계 프로세스를 따릅니다:\n",
        "쿼리 처리: 사용자의 질문을 분석하고 검색에 적합한 형식으로 변환합니다.\n",
        "정보 검색: 처리된 쿼리를 사용하여 외부 지식 베이스에서 관련 문서나 텍스트 조각을 검색합니다.\n",
        "컨텍스트 증강: 검색된 정보를 원래 질문과 결합하여 증강된 프롬프트를 생성합니다.\n",
        "답변 생성: 증강된 프롬프트를 LLM에 입력하여 질문과 검색된 컨텍스트를 모두 기반으로 응답을 생성합니다.\n",
        "RAG의 주요 구성 요소\n",
        "\n",
        "1. 문서 인덱싱\n",
        "RAG를 사용하기 전에 외부 지식 베이스를 적절히 인덱싱해야 합니다:\n",
        "문서를 관리 가능한 크기의 조각으로 분할합니다.\n",
        "각 조각을 적절한 모델을 사용하여 벡터 임베딩으로 변환합니다.\n",
        "이 임베딩을 효율적인 검색을 위해 벡터 데이터베이스에 저장합니다.\n",
        "\n",
        "2. 검색기\n",
        "검색기는 관련 정보를 찾는 역할을 합니다:\n",
        "사용자의 질문을 벡터 임베딩으로 변환합니다.\n",
        "벡터 데이터베이스에서 유사도 검색을 수행하여 가장 관련성 높은 텍스트 조각을 찾습니다.\n",
        "상위 k개의 가장 유사한 조각을 반환합니다.\n",
        "\n",
        "3. 생성기\n",
        "생성기는 일반적으로 사전 훈련된 LLM으로:\n",
        "원래 질문과 검색된 컨텍스트를 입력으로 받습니다.\n",
        "이 증강된 프롬프트를 기반으로 일관성 있는 답변을 생성합니다.\n",
        "질문 답변에서 RAG의 장점\n",
        "향상된 정확성: 관련 컨텍스트를 제공함으로써 RAG는 환각을 줄이고 응답의 사실적 정확성을 향상시킵니다.\n",
        "최신 정보: 외부 지식 베이스를 쉽게 업데이트할 수 있어, 전체 모델을 재훈련하지 않고도 시스템이 최신 정보에 접근할 수 있습니다.\n",
        "도메인 적응: RAG를 통해 LLM은 관련 문서를 인덱싱하는 것만으로도 특정 도메인이나 독점 정보에 대한 질문에 답할 수 있습니다.\n",
        "투명성: 검색된 컨텍스트를 생성된 답변과 함께 보여줄 수 있어, 명확한 정보 출처를 제공합니다."
      ],
      "metadata": {
        "id": "7Tz7hBJwYUNf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "faUslVhCQDid",
        "outputId": "312fd383-0d32-4ad3-d97b-ff40c5a7589f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 32768 tokens. However, your messages resulted in 36214 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-90315002ad20>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_with_history_prompt\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mStrOutputParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mquery1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Performance comparison amongst the merge candidate\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mresponse1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"history\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RESPONSE1\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         return cast(\n\u001b[1;32m    285\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    785\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         flattened_outputs = [\n\u001b[1;32m    645\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 results.append(\n\u001b[0;32m--> 633\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    634\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    852\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_upstage/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_request_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    813\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    814\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    816\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         )\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    955\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 32768 tokens. However, your messages resulted in 36214 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
          ]
        }
      ],
      "source": [
        "# More general chat\n",
        "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"\n",
        "You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question considering the history of the conversation.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "---\n",
        "CONTEXT:\n",
        "{context}\n",
        "         \"\"\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "history = []\n",
        "\n",
        "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
        "query1 = \"Performance comparison amongst the merge candidate\"\n",
        "response1 = chain.invoke({\"history\": history, \"context\": docs, \"input\": query1})\n",
        "print(\"RESPONSE1\\n\", response1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드는 LangChain을 사용하여 RAG(검색 보강 생성) 방식으로 대화형 질문 응답 시스템을 구현합니다. 대화의 히스토리와 검색된 맥락(context)을 바탕으로 질문에 답변을 생성합니다. 각 단계별 설명은 아래와 같습니다.\n",
        "\n",
        "코드 설명\n",
        "프롬프트 템플릿 설정:\n",
        "\n",
        "ChatPromptTemplate.from_messages()를 사용해 RAG 기반 대화형 질문 응답 프롬프트 템플릿을 생성합니다.\n",
        "system 메시지에서는 질문에 대한 답변을 생성할 때 검색된 맥락을 참조하고, 대화 히스토리를 고려하도록 모델에 지시합니다.\n",
        "{context}와 {input}는 각각 검색된 정보와 사용자의 질문을 받아와 모델에 전달될 프롬프트 내용입니다.\n",
        "대화 히스토리 관리:\n",
        "\n",
        "history 리스트는 대화의 히스토리를 저장하는 용도로 사용됩니다.\n",
        "AIMessage와 HumanMessage 클래스를 사용해 이전 대화 내용을 기록하고, 히스토리로 전달할 수 있습니다.\n",
        "체인 생성 및 실행:\n",
        "\n",
        "chain = rag_with_history_prompt | llm | StrOutputParser()로 프롬프트 템플릿과 언어 모델(LLM), 그리고 출력 파서를 연결한 체인을 생성합니다.\n",
        "chain.invoke()를 통해 사용자의 질문과 맥락을 전달하고, 응답을 생성합니다.\n",
        "첫 번째 질문으로 \"Performance comparison amongst the merge candidate\"를 입력하고, 이를 query1에 저장하여 응답을 받습니다.\n",
        "응답 출력:\n",
        "\n",
        "print(\"RESPONSE1\\n\", response1)를 사용해 첫 번째 질문에 대한 모델의 응답을 출력합니다."
      ],
      "metadata": {
        "id": "mUH5y_Re2kQ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH69Jc1eQDid",
        "outputId": "3225611f-88ad-49f5-dcf7-8c4f1b386e4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESPONSE2\n",
            " Ablation studies are techniques used in research to understand the impact of a specific component or change in a system. In the context of the document, ablation studies are conducted to understand the effect of different training datasets, base models, and merge methods on the performance of the Large Language Model (LLM).\n",
            "\n",
            "The document presents ablation studies in two main areas: Instruction Tuning and Alignment Tuning.\n",
            "\n",
            "1. Instruction Tuning: Ablation studies on instruction tuning involve testing the performance of the model when trained with different combinations of training datasets. The study compares the performance of the model when trained with different datasets, such as Alpaca-GPT4, OpenOrca, and Synth. Math-Instruct. The results show that adding the Synth. Math-Instruct dataset to the training process improves the model's performance on the GSM8K task.\n",
            "2. Alignment Tuning: Ablation studies on alignment tuning involve testing the performance of the model when trained with different combinations of training datasets and base models. The study compares the performance of the model when trained with different combinations of Ultrafeedback Clean, Synth. Math-Alignment, and different base models. The results show that adding Synth. Math-Alignment to the training process improves the model's performance on the GSM8K task.\n",
            "\n",
            "The ablation studies provide valuable insights into the factors that influence the performance of the LLM. By understanding the impact of different training datasets, base models, and merge methods, researchers can make informed decisions about the best approach to train and fine-tune the model for specific tasks.\n"
          ]
        }
      ],
      "source": [
        "history = [HumanMessage(query1), AIMessage(response1)]\n",
        "query2 = \"How about Ablation studies?\"\n",
        "response2 = chain.invoke({\"history\": history, \"context\": docs, \"input\": query2})\n",
        "print(\"RESPONSE2\\n\", response2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드는 대화의 히스토리를 유지하면서 두 번째 질문을 모델에 전달하여 응답을 생성합니다. 첫 번째 질문과 응답을 히스토리에 추가하고, 두 번째 질문에 대한 답변을 생성하는 방식입니다.\n",
        "\n",
        "코드 설명\n",
        "히스토리 업데이트:\n",
        "\n",
        "첫 번째 질문(query1)과 그에 대한 응답(response1)을 각각 HumanMessage와 AIMessage로 생성하여 history 리스트에 추가합니다.\n",
        "history = [HumanMessage(query1), AIMessage(response1)]는 이전 대화의 히스토리를 저장하여, 모델이 두 번째 질문을 처리할 때 이를 참조할 수 있게 합니다.\n",
        "두 번째 질문 실행:\n",
        "\n",
        "query2에 두 번째 질문 \"How about Ablation studies?\"를 저장하고, 이를 chain.invoke()를 통해 전달합니다.\n",
        "모델은 history에 저장된 대화 내용을 참고하여, 문맥에 맞는 답변을 제공합니다.\n",
        "응답 출력:\n",
        "\n",
        "print(\"RESPONSE2\\n\", response2)를 통해 두 번째 질문에 대한 모델의 응답을 출력합니다."
      ],
      "metadata": {
        "id": "3E2uoRpI24T2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjl0uYgkQDid"
      },
      "source": [
        "### 3-3  RAG Limitations\n",
        "- LLM does not have long enough context length\n",
        "- Sending long, irrelevant info is inefficient"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM의 제한된 컨텍스트 길이\n",
        "\n",
        "문제점:\n",
        "LLM(Large Language Models)은 한 번에 처리할 수 있는 텍스트의 길이에 제한이 있습니다.\n",
        "대부분의 모델은 2048, 4096 또는 8192 토큰 정도의 컨텍스트 길이를 가집니다.\n",
        "\n",
        "영향:\n",
        "긴 문서나 복잡한 질문-답변 세션에서 모든 관련 정보를 한 번에 처리하기 어렵습니다.\n",
        "컨텍스트 길이를 초과하는 정보는 무시되거나 잘릴 수 있습니다.\n",
        "\n",
        "해결 방안:\n",
        "문서를 더 작은 청크로 분할하고 가장 관련성 높은 부분만 선택합니다.\n",
        "다중 단계 추론 또는 체인 형태의 질문-답변 프로세스를 구현합니다."
      ],
      "metadata": {
        "id": "nMDD-Cikaun7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRT_mkBvQDie"
      },
      "outputs": [],
      "source": [
        "# Let's load something big\n",
        "layzer = UpstageLayoutAnalysisLoader(\n",
        "    \"/content/kim-tse-2008.pdf\", output_type=\"html\", use_ocr=True\n",
        ")\n",
        "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
        "docs = layzer.load()  # or layzer.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7zuyMtJQDie",
        "outputId": "de077b75-c3f0-402e-fc9b-29dbdcd54e80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error code: 400 - {'error': {'message': \"This model's maximum context length is 32768 tokens. However, your messages resulted in 36212 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
          ]
        }
      ],
      "source": [
        "# More general chat\n",
        "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"\n",
        "You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question considering the history of the conversation.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "---\n",
        "CONTEXT:\n",
        "{context}\n",
        "         \"\"\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = rag_with_history_prompt | llm | StrOutputParser\n",
        "()\n",
        "query1 = \"What is bug classification?\"\n",
        "\n",
        "try:\n",
        "    response1 = chain.invoke({\"history\": history, \"context\": docs, \"input\": query1})\n",
        "    print(response1)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwNO1dgZQDie",
        "outputId": "f60cb048-1d56-4009-e034-0587d96626ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107727\n"
          ]
        }
      ],
      "source": [
        "print(len(docs[0].page_content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f25efaeb5c5b45b2a36633b0128d3b48",
            "4a146b57d8834ce19153ef83efc50b0c",
            "2e6d1777ea4a47d19ad68c76e3e5ebe1",
            "5d7e4fd59abe48729c4d5b0431c823f9",
            "f4ac7d27b215458087b5f99f8c839cc0",
            "a4c945b3ff644468b3f5aa9f983c7281",
            "631ae0ccb8a94ceb89a8b66b6540f873",
            "b4f72e0f65574313a2c6b146e2d73376",
            "657b71996e044b6a8812447568835478",
            "f73fd1da56a04d0e95a73bf7b34d9f27",
            "95e94bef499b4e1ab34eaf1c57a8c921"
          ]
        },
        "id": "cDrzWMG3QDie",
        "outputId": "ae7a59cc-7706-4039-f17e-4b92724070ef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/3.31M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f25efaeb5c5b45b2a36633b0128d3b48"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer.from_pretrained(\"upstage/solar-1-mini-tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from tokenizers import Tokenizer\n",
        "tokenizers 라이브러리에서 Tokenizer 클래스를 가져옵니다.\n",
        "이 라이브러리는 효율적인 토큰화를 위한 다양한 도구를 제공합니다.\n",
        "\n",
        "tokenizer = Tokenizer.from_pretrained(\"upstage/solar-1-mini-tokenizer\")\n",
        "Tokenizer.from_pretrained() 메서드를 사용하여 사전 훈련된 토크나이저를 로드합니다.\n",
        "\n",
        "\"upstage/solar-1-mini-tokenizer\"는 Hugging Face 모델 허브에서 제공하는 Upstage의 Solar-1-Mini 토크나이저의 식별자입니다.\n",
        "\n",
        "이 토크나이저의 주요 특징:\n",
        "어휘 크기: 64,000\n",
        "지원 언어: 영어, 한국어, 일본어 등 다국어 지원\n",
        "Upstage solar-1-mini-chat 모델용으로 설계됨\n",
        "토크나이저를 로드한 후에는 다음과 같은 작업을 수행할 수 있습니다:\n",
        "텍스트 인코딩: tokenizer.encode(text)\n",
        "토큰 디코딩: 인코딩된 ID를 다시 토큰으로 변환\n",
        "토큰 수 계산: 인코딩된 텍스트의 토큰 수 확인"
      ],
      "metadata": {
        "id": "Z7yJxOGgemSS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZqqOOimQDif",
        "outputId": "c91a36d3-cd52-4e1e-952a-a814c542debc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded input: ['<|startoftext|>', '▁Nice', '▁to', '▁meet', '▁you', '.', '▁I', '▁am', '▁Solar', '▁LL', 'M', ',', '▁a', '▁large', '▁language', '▁model', '▁developed', '▁by', '▁Up', 'stage', '.', '▁If', '▁you', '▁have', '▁any', '▁questions', ',', '▁please', '▁feel', '▁free', '▁to', '▁ask', '.']\n",
            "Number of tokens: 33\n"
          ]
        }
      ],
      "source": [
        "text = \"Nice to meet you. I am Solar LLM, a large language model developed by Upstage. If you have any questions, please feel free to ask.\"\n",
        "\n",
        "enc = tokenizer.encode(text)\n",
        "print(\"Encoded input:\", enc.tokens)\n",
        "\n",
        "number_of_tokens = len(enc.tokens)\n",
        "print(\"Number of tokens:\", number_of_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "토큰화 과정에서 단어나 부분 단어로 텍스트가 분할됩니다.\n",
        "'Ġ' 기호는 단어 앞의 공백을 나타냅니다.\n",
        "구두점(., ,)도 별도의 토큰으로 처리됩니다.\n",
        "\"Solar LLM\"과 \"Upstage\"같은 특정 용어들이 개별 토큰으로 인식됩니다.\n",
        "이 토큰화 결과를 통해 모델이 텍스트를 어떻게 이해하고 처리하는지 알 수 있으며, 입력 텍스트의 길이를 토큰 단위로 측정"
      ],
      "metadata": {
        "id": "OkQnZ_9ZfCkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gLtwddoQDif",
        "outputId": "9c654a9c-a92e-4bfa-9b5a-901b91fd9d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded input: ['<|startoftext|>', '▁만나', '서', '▁반가', '워', '요', '.', '▁저는', '▁Up', 'stage', '에서', '▁개발한', '▁대규모', '▁언어', '▁모델', '인', '▁Solar', '▁LL', 'M', '▁입니다', '.', '▁궁금한', '▁것이', '▁있으', '시면', '▁무엇이', '든', '▁물어', '보세요', '.']\n",
            "Number of tokens: 30\n"
          ]
        }
      ],
      "source": [
        "text = \"만나서 반가워요. 저는 Upstage에서 개발한 대규모 언어 모델인 Solar LLM 입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\"\n",
        "enc = tokenizer.encode(text)\n",
        "print(\"Encoded input:\", enc.tokens)\n",
        "\n",
        "number_of_tokens = len(enc.tokens)\n",
        "print(\"Number of tokens:\", number_of_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CBIRWVAQDif"
      },
      "outputs": [],
      "source": [
        "def num_of_tokens(text):\n",
        "    return len(tokenizer.encode(text).tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfkBy7bpQDif",
        "outputId": "f8aaffda-cc3e-48d9-e639-20bc50134e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENG 33\n",
            "KOR 30\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    \"ENG\",\n",
        "    num_of_tokens(\n",
        "        \"Nice to meet you. I am Solar LLM, a large language model developed by Upstage. If you have any questions, please feel free to ask.\"\n",
        "    ),\n",
        ")\n",
        "print(\n",
        "    \"KOR\",\n",
        "    num_of_tokens(\n",
        "        \"만나서 반가워요. 저는 Upstage에서 개발한 대규모 언어 모델인 Solar LLM 입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCeqDE-MQDig",
        "outputId": "726e76d4-e80a-4ee9-ce0d-b9e4afc61805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String length 107727\n",
            "Number of tokens 35128\n"
          ]
        }
      ],
      "source": [
        "# Recall\n",
        "# Let's load something big\n",
        "# layzer = UpstageLayoutAnalysisLoader(\"pdfs/kim-tse-2008.pdf\", output_type=\"html\")\n",
        "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
        "# docs = layzer.load()  # or layzer.lazy_load()\n",
        "print(\"String length\", len(docs[0].page_content))\n",
        "print(\"Number of tokens\", num_of_tokens(docs[0].page_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBIiRaKlQDih"
      },
      "source": [
        "## [Session 4] Efficient Text Splitting and Indexing with LangChain\n",
        "\n",
        "\n",
        "### Steps\n",
        "<b> 1. Load Documents </b>\n",
        "\n",
        "The first step is to load the source documents that will be used to augment the language model's knowledge\n",
        "This could be done by reading files from disk, pulling from a database, scraping web pages, etc.\n",
        "The goal is to get the raw text content into a format that can be further processed\n",
        "\n",
        "<b>2. Chunking/Splitting</b>\n",
        "\n",
        "* Long documents need to be broken down into smaller chunks that are a manageable size for embedding and retrieval\n",
        "Common approaches include:\n",
        "  * Fixed-size chunking - split text into equal sized chunks based on character or token count\n",
        "  * Semantic chunking - split based on semantic boundaries like sentences, paragraphs, or sections\n",
        "  * Hierarchical chunking - create chunks at multiple levels of granularity\n",
        "The ideal chunk size depends on the embedding model, retrieval use case, and downstream task\n",
        "\n",
        "<b>3. Embedding & Indexing</b>\n",
        "\n",
        "* The text chunks are converted to vector embeddings using a model like Upstage embeddings\n",
        "* The embeddings are indexed and stored in a vector database to enable efficient similarity search\n",
        "* Metadata about the source chunks can also be stored alongside the embeddings\n",
        "\n",
        "<b>4. Retrieval</b>\n",
        "\n",
        "* At query time, the user's question is itself embedded as a query vector\n",
        "* The query embedding is used to find the most similar document chunks in the vector index\n",
        "* Top-k most relevant chunks are retrieved and can be used to augment the prompt sent to the language model to generate an answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Efficient Text Splitting and Indexing with LangChain은 언어 모델이 큰 문서에서 정보를 효율적으로 검색할 수 있도록 문서 분할과 벡터화(Embedding) 작업을 수행하는 프로세스를 설명합니다.\n",
        "\n",
        "Load Documents (문서 로드):\n",
        "\n",
        "먼저 언어 모델의 지식을 보강할 수 있는 소스 문서들을 로드합니다. 문서들은 파일, 데이터베이스, 웹 스크래핑 등을 통해 가져올 수 있습니다.\n",
        "목표는 문서 내용을 추출하여 이후 단계에서 처리 가능한 형태의 텍스트로 준비하는 것입니다.\n",
        "Chunking/Splitting (문서 분할):\n",
        "\n",
        "긴 문서를 임베딩과 검색에 적합하도록 작은 청크로 나눕니다.\n",
        "일반적인 분할 방법은 다음과 같습니다:\n",
        "Fixed-size chunking: 문자 수나 토큰 수를 기준으로 고정된 크기로 나누기.\n",
        "Semantic chunking: 문장, 단락, 섹션 등의 의미 단위로 나누기.\n",
        "Hierarchical chunking: 여러 수준에서 청크를 만들어 더 세밀하게 분할.\n",
        "적절한 청크 크기는 사용되는 임베딩 모델, 검색 목적, 이후 작업에 따라 다릅니다. 일반적으로 청크가 너무 작거나 너무 크면 효율성이 떨어질 수 있습니다.\n",
        "Embedding & Indexing (임베딩 및 인덱싱):\n",
        "\n",
        "나눈 텍스트 청크는 벡터 임베딩으로 변환되어, 숫자 벡터 형태로 저장됩니다. Upstage 임베딩 모델 같은 것을 사용해 벡터화할 수 있습니다.\n",
        "임베딩 벡터는 벡터 데이터베이스에 인덱싱하여 저장합니다. 이렇게 하면 유사도 검색이 빠르게 이루어질 수 있습니다.\n",
        "원본 청크의 메타데이터도 임베딩과 함께 저장하여 나중에 검색 시 참고할 수 있습니다.\n",
        "Retrieval (검색):\n",
        "\n",
        "질문이 들어오면, 질문을 임베딩하여 쿼리 벡터로 변환합니다.\n",
        "쿼리 벡터를 사용해 벡터 인덱스에서 가장 유사한 문서 청크들을 검색합니다.\n",
        "관련성이 높은 상위 K개의 청크를 검색하여, 이를 언어 모델에 프롬프트로 전달해 답변 생성을 보강할 수 있습니다.\n",
        "\n",
        "\n",
        "-> LangChain을 통해 문서를 로드하고 작은 청크로 나눈 뒤 벡터화하여 벡터 데이터베이스에 인덱싱합니다.\n",
        "사용자의 질문이 들어오면 유사도 검색을 통해 가장 관련성이 높은 청크를 찾아 응답을 생성합니다.\n",
        "이 프로세스를 통해 긴 문서에서도 효율적으로 정보를 검색하고, 언어 모델의 응답 정확도를 높일 수 있습니다."
      ],
      "metadata": {
        "id": "wf0J94pJstFt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1df5wN-BQDii"
      },
      "source": [
        "#### RecursiveCharacterTextSplitter\n",
        "\n",
        " `RecursiveCharacterTextSplitter` class is designed to be recursively split so that semantically related pieces remain together. <br>\n",
        " During this process, a list of delimiter characters `(['\\n\\n', '\\n', ' ', ''])` is used sequentially to partition the text.\n",
        "- This splitting continues until the resulting chunks are smaller than the specified `chunk_size`.\n",
        "- The `chunk_overlap` parameter defines the number of characters that should overlap between the divided text chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RecursiveCharacterTextSplitter는 구분자 리스트와 chunk_overlap 설정을 활용하여 텍스트를 의미가 유지되면서도 작은 청크로 나눌 수 있게 해줍니다. 이는 대형 언어 모델이 텍스트의 의미를 충분히 이해할 수 있는 청크 단위로 데이터를 제공하는 데 유용한 방법입니다."
      ],
      "metadata": {
        "id": "RcmOv5o7tTSA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hLVM-cNDQDii"
      },
      "outputs": [],
      "source": [
        "# RAG 1. load doc (done), 2. chunking, splits, 3. embeding - indexing, 4. retrieve\n",
        "\n",
        "# layzer = UpstageLayoutAnalysisLoader(\"pdfs/kim-tse-2008.pdf\", output_type=\"html\")\n",
        "# # For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
        "# docs = layzer.load()  # or layzer.lazy_load()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 문서 로드\n",
        "이 단계는 이미 완료된 상태입니다. UpstageLayoutAnalysisLoader를 사용해 PDF 파일을 불러와서 HTML 형식으로 로드한 후, docs 변수에 저장합니다.\n",
        "\n",
        "2. 청크로 나누기 (Chunking & Splitting)\n",
        "문서가 길 경우 적절한 크기로 청크를 나눠야 합니다. RecursiveCharacterTextSplitter와 같은 도구를 사용해, 구분자에 따라 텍스트를 의미 단위로 나눌 수 있습니다.\n",
        "\n",
        "3. 임베딩 및 인덱싱 (Embedding & Indexing)\n",
        "청크된 텍스트를 임베딩 벡터로 변환한 후, 벡터 데이터베이스에 저장하여 나중에 검색할 수 있도록 합니다. 여기서는 LangChain을 활용한 벡터 데이터베이스와 임베딩을 설정할 수 있습니다.\n",
        "\n",
        "4. 검색 및 응답 생성 (Retrieve)\n",
        "사용자의 질문을 받아 쿼리 임베딩을 생성한 후, 벡터 데이터베이스에서 가장 유사한 청크를 찾아 응답을 생성합니다."
      ],
      "metadata": {
        "id": "IMheCyYBt-Hq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "3jGNm1yiQDii",
        "outputId": "d807696c-bfdd-4b41-b9f3-e24f9a6350b0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'RecursiveCharacterTextSplitter' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-52cf240ac1d7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2. Split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext_splitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Splits:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RecursiveCharacterTextSplitter' is not defined"
          ]
        }
      ],
      "source": [
        "# 2. Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(\"Splits:\", len(splits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "YBme6bLNQDij",
        "outputId": "acc23419-f888-428a-9820-f8c7ea2fd2ff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'splits' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e426e848cb11>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msplit_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create a bar graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'splits' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "split_lengths = [len(split.page_content) for split in splits]\n",
        "\n",
        "# Create a bar graph\n",
        "plt.bar(range(len(split_lengths)), split_lengths)\n",
        "plt.title(\"RecursiveCharacterTextSplitter\")\n",
        "plt.xlabel(\"Split Index\")\n",
        "plt.ylabel(\"Split Content Length\")\n",
        "plt.xticks(range(len(split_lengths)), [])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 시각화를 통해 청크의 크기 분포를 쉽게 확인할 수 있어, 청크가 너무 크거나 작게 나뉘지는 않았는지 검토할 수 있습니다."
      ],
      "metadata": {
        "id": "uiUJweLTueAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "Wj9oAiOCQDik",
        "outputId": "b75f7bd0-c80d-4fad-ee5a-a164065572b2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Chroma' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Chroma' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# 3. Embed & indexing\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "임베딩 모델 선택:\n",
        "\n",
        "UpstageEmbeddings(model=\"solar-embedding-1-large\")를 사용하여 solar-embedding-1-large라는 임베딩 모델을 지정합니다.\n",
        "이 모델은 문서 청크를 벡터 형태로 변환하여, 의미적으로 유사한 텍스트를 가까운 벡터 위치에 매핑하는 역할을 합니다.\n",
        "벡터 스토어 생성 및 인덱싱:\n",
        "\n",
        "Chroma.from_documents()를 통해, 임베딩 벡터를 벡터 데이터베이스에 인덱싱합니다.\n",
        "documents=splits는 이전 단계에서 분할한 문서 청크를 사용하며, 각 청크가 벡터화되어 저장됩니다.\n",
        "이 벡터 데이터베이스는 유사도 검색을 통해 질문과 관련된 청크를 빠르게 찾을 수 있도록 도와줍니다."
      ],
      "metadata": {
        "id": "wzRBaqLFuqxi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNvIyqctQDik",
        "outputId": "89fdb0ea-edeb-4860-83a8-50aaca1833e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "<h1 id='2' style='font-size:22px'>Classifying Software Changes:<br>Clean or Buggy?</h1><br><p id='3'\n"
          ]
        }
      ],
      "source": [
        "# 4. retrive\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "result_docs = retriever.invoke(\"What is Bug Classification?\")\n",
        "print(len(result_docs))\n",
        "print(result_docs[0].page_content[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retriever 생성:\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})는 vectorstore를 기반으로 retriever 객체를 생성합니다.\n",
        "search_kwargs={\"k\": 3}는 상위 3개의 관련 청크를 검색하도록 설정합니다. 이를 통해 가장 관련성이 높은 청크 3개가 반환됩니다.\n",
        "질문에 대한 검색 수행:\n",
        "\n",
        "retriever.invoke(\"What is Bug Classification?\")를 통해, 질문 **\"What is Bug Classification?\"**에 해당하는 관련 청크를 검색합니다.\n",
        "검색된 결과는 result_docs 변수에 저장됩니다.\n",
        "결과 출력:\n",
        "\n",
        "print(len(result_docs))를 통해 검색된 문서 수를 출력합니다.\n",
        "print(result_docs[0].page_content[:100])를 통해, 첫 번째 결과 문서의 내용 중 앞부분 100자를 확인합니다."
      ],
      "metadata": {
        "id": "FJgOtHppvCjE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okdP0YyWQDik"
      },
      "source": [
        "#### SemanticChunker\n",
        "\n",
        "SemanticChunker is an experimental feature in LangChain that serves to split text into semantically similar chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SemanticChunker는 의미 기반의 청크 생성을 통해 텍스트를 분할하는 LangChain의 기능입니다. 이를 통해 긴 문서를 의미 단위로 나눌 수 있어, 검색과 응답 생성에서 높은 정확도를 기대할 수 있습니다."
      ],
      "metadata": {
        "id": "hWm2J9f0wDKP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh235QHaQDik"
      },
      "source": [
        "![Overview](https://github.com/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/figures/semantic_chunker.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acUAei5UQDik"
      },
      "outputs": [],
      "source": [
        "# 2-2. SemanticChunker Split\n",
        "from langchain_community.utils.math import cosine_similarity\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "\n",
        "def semantic_chunker(\n",
        "    docs,\n",
        "    min_chunk_size=100,\n",
        "    chunk_overlap=10,\n",
        "    max_chunk_size=1000,\n",
        "    merge_threshold=0.7,\n",
        "    embeddings=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
        "):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=min_chunk_size, chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    init_splits = text_splitter.split_documents(docs)\n",
        "    splits = []\n",
        "\n",
        "    base_split_text = None\n",
        "    base_split_emb = None\n",
        "    for split in init_splits:\n",
        "        if base_split_text is None:\n",
        "            base_split_text = split.page_content\n",
        "            base_split_emb = embeddings.embed_documents([base_split_text])[0]\n",
        "            continue\n",
        "\n",
        "        split_emb = embeddings.embed_documents([split.page_content])[0]\n",
        "        distance = cosine_similarity(X=[base_split_emb], Y=[split_emb])\n",
        "        if (\n",
        "            distance[0][0] < merge_threshold\n",
        "            or len(base_split_text) + len(split.page_content) > max_chunk_size\n",
        "        ):\n",
        "            splits.append(Document(page_content=base_split_text))\n",
        "            base_split_text = split.page_content\n",
        "            base_split_emb = split_emb\n",
        "        else:\n",
        "            base_split_text += split.page_content\n",
        "\n",
        "    if base_split_text:\n",
        "        splits.append(Document(page_content=base_split_text))\n",
        "\n",
        "    return splits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 semantic_chunker 함수는 초기 청크로 나눈 후, 유사한 청크끼리 결합하여 의미를 유지하면서 최적의 청크 크기로 나눕니다. 이를 통해 문서의 문맥적 일관성을 유지하고, 유사한 내용이 포함된 청크를 생성하여 검색 및 응답 정확도를 높일 수 있습니다."
      ],
      "metadata": {
        "id": "P3A-Y1YywMR9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijkbp7wUQDik"
      },
      "source": [
        "#### HuggingFaceEmbeddings\n",
        "Since it's just an approximation, it's acceptable to use very light embedding models like KLUE, https://huggingface.co/klue."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HuggingFaceEmbeddings는 Hugging Face의 임베딩 모델을 활용하여 텍스트를 벡터로 변환하는 기능입니다. RAG 파이프라인에서 문서 청크나 질문을 벡터화하여, 검색 및 유사도 비교 작업에 사용할 수 있습니다. KLUE와 같은 경량 모델을 활용하면, 빠르고 효율적으로 임베딩을 생성할 수 있습니다."
      ],
      "metadata": {
        "id": "HWcZwEd1yvym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "39ce5e4c64934c7ba2df82ad1e4f1498",
            "493aa6d96f7e4ce0bb91666f28d5001d",
            "7be12a97a5af4ad98ba581382d933243",
            "b1d507020abc4873bfdc14dcda8193c2",
            "ad7416be07a1466ca82f9f4a93b4c21e",
            "4c9dd4908c794969bcc9e4c39c0e47f7",
            "bfc5d2260e484de2be4110d292ddbce0",
            "2b537937fc314b8d8eb319ec2244cf5b",
            "2c1185dc55ef4f85a4bfe74a9755bdac",
            "ba069a59de7e4e65947bcc81ca6ba947",
            "0e20f1659ced4708902011d66748ea7a",
            "3c3ffd90820b4fbcb4c33b3ba5110596",
            "c1afb1d971304e95a68811c4762698b0",
            "fb4d9944561f4678aeeaa7a24d32826c",
            "0f8c225633564210bd7b8dd9c5035049",
            "9bbe32ab4e434b64879bc858c2f58dab",
            "a708818f5363498da9afa8300c632026",
            "da0f3c009ffe4ed1a09a798152da622d",
            "c0d30de80b354150891d4370725d8289",
            "f216c978eb2946bcb6de1497c9b5ee66",
            "8db6ccbb17ed40cab842256122c98bed",
            "995e89b93ea8467084ee9c9616fb370e",
            "b8496a8f7fd746a58ab0ab38a614e70e",
            "a174ca6d295648e3a3fa0666aa5483a6",
            "2ec1a35c4ca84727a21d8d2dd675b378",
            "74afa7401cbc41b3805a3fd74efef7a4",
            "33f483ed5ac446439d30adc2ac9e04c3",
            "b68d226b84ec43e0b0751129d80d7eaf",
            "fd32b39270934512be7f742b269ab76c",
            "b1a6aacc9cb5433ea39b5dface27a11f",
            "b4ef6a5028734105ad354669054a4722",
            "814c8d9c524d4b52a7c30d5b101f8013",
            "e13873faabb74a2d990c03ca171c7fa8",
            "da060a1b2f554181921b6f21445ef667",
            "09f9fd2d872d4d1ba3982187073a59b7",
            "b7bbb64b314e455981cbf396a477122f",
            "e350da9ea90a4605b14afc66e43e8d66",
            "fc22a313f47b490eae4c52c56a79b5f4",
            "217cdf372f304f7986824dc289adfe51",
            "a45ef0c2a78d482ba0ee7352b4308d72",
            "ec3ffe596ea642b59bdce97facf82afc",
            "d991a9a7f3214402ab45b29954e4031b",
            "7d775ae7dd024dc0b457e0c7971e9f07",
            "97b2714b768b4928997402a786fc2d24",
            "b086a0e8c22d4a27b7225f4fa617e1c4",
            "2172d2509ede41e1a871bb66b0eb4ecd",
            "89096d8969314395b18a928e28364047",
            "1dd4a9b196594c4c9803ed7e47fab580",
            "04ae0fdf6dd84219b7f8440d1abd5c72",
            "23e144bde5bc427a8529062ccd1c60e5",
            "dcfa07e2c99e4598a3aecbb32632b859",
            "0cd27ff43fab48fdb7478b2791524cf2",
            "ecbbda04ab80494aa09c79d3eef19bc3",
            "e277eb77d12348fb892f94e576e9de65",
            "43b3e296e56b4245af5fc6a013189e5b",
            "1c0f4af089054804a4bf4f71e9108895",
            "fa4baedd966346b0a840b237f8fcb6ba",
            "15a9632bba6a42d68768c1c7932d5fd1",
            "37d479a484e64154b921941bcbd8877a",
            "dad776c665b64ef4a2289bb01fe2ae01",
            "824d8364c4294db5aba39ef680e73a8d",
            "888f0347f0aa488b951d53b27c6edecf",
            "b163ecf4cfc449a4b10743e08bce0736",
            "6db847034f2c455e99436e2970f6813f",
            "739c6d38b015434d9f646c16ebf96570",
            "833655151f14441a9567760e1d883ea1",
            "aa64a131b11840f6a4e5161187cdff65",
            "a63cb9a42b8341e8a58737c42aa575be",
            "2db720073c8643c99ff382d28e8a2eda",
            "04cc8b20f54149db99a2391109bb5e01",
            "c0c35a88ade0430582903b32ae276633",
            "501c347c2fb94d988aacb318d40d0141",
            "ed356d80b19445a2b7ef30e8e2ecf938",
            "840a339c1a5d432fa9478aa7930cad24",
            "21caaf4e1b7c48ea9d9137f26deb5baf",
            "ba5a1f3aa6c548edaa21e53469716151",
            "d8d530d5178c45a0bb6593c19d6ce421"
          ]
        },
        "id": "MCq8tfxHQDil",
        "outputId": "f3233701-3559-4404-f5be-d62967340f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39ce5e4c64934c7ba2df82ad1e4f1498"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name klue/roberta-small. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/545 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c3ffd90820b4fbcb4c33b3ba5110596"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/273M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8496a8f7fd746a58ab0ab38a614e70e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da060a1b2f554181921b6f21445ef667"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b086a0e8c22d4a27b7225f4fa617e1c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/752k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c0f4af089054804a4bf4f71e9108895"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa64a131b11840f6a4e5161187cdff65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17.7 s, sys: 4.13 s, total: 21.9 s\n",
            "Wall time: 32.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "hfembeddings = HuggingFaceEmbeddings(model_name=\"klue/roberta-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjJs0UJnQDil",
        "outputId": "058a99a1-07bd-4be2-c80b-45a907d41c49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SemanticChunker Splits: 248\n",
            "CPU times: user 2min 57s, sys: 188 ms, total: 2min 57s\n",
            "Wall time: 3min 1s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "semantic_splits = semantic_chunker(docs, merge_threshold=0.8, embeddings=hfembeddings)\n",
        "print(\"SemanticChunker Splits:\", len(semantic_splits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "tvPqYOKXQDil",
        "outputId": "10306c34-53e6-44b4-a102-5fecb9082dfb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "split_lengths = [num_of_tokens(split.page_content) for split in semantic_splits]\n",
        "\n",
        "# Create a bar graph\n",
        "plt.bar(range(len(split_lengths)), split_lengths)\n",
        "plt.xlabel(\"Split Index\")\n",
        "plt.ylabel(\"Split Content Length\")\n",
        "plt.title(\"Semantic Chunker Split Page Content Lengths\")\n",
        "plt.xticks(range(len(split_lengths)), [])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFc9iPahQDil"
      },
      "source": [
        "### ChromaParallel Class: Parallel Document Embedding\n",
        "The ChromaParallel class is an extension of the Chroma class to enable parallel processing of document embedding and storage using multiple worker processes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChromaParallel 클래스는 대량의 문서를 병렬로 임베딩하고 저장하는 데 최적화된 Chroma의 확장판입니다. 이를 사용하면 대규모 문서 처리 작업에서 속도와 효율성을 높일 수 있으며, from_documents 메서드를 통해 문서를 벡터화하여 검색에 유용한 벡터 데이터베이스를 생성할 수 있습니다."
      ],
      "metadata": {
        "id": "aOJLa3tp3dy5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p29TYsq2QDil"
      },
      "source": [
        "Chroma is an AI-native open source vector database designed to enhance developer productivity and satisfaction. It is licensed under the Apache 2.0 license.\n",
        "\n",
        "- <b> Generate Vectorspace </b> : The `from_documents` class method creates a vector store from a list of documents.\n",
        "\n",
        "##### Reference\n",
        "\n",
        "* [Chroma LangChain Documentation](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n",
        "* [Chroma Official Documentation](https://docs.trychroma.com/getting-started)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chroma는 AI 관련 벡터 데이터를 효율적으로 저장하고 검색하는 오픈 소스 벡터 데이터베이스로, from_documents 메서드를 통해 간단하게 벡터 스토어를 생성할 수 있습니다. 이를 통해 질문 응답, 추천 시스템, 문서 검색 등 다양한 AI 응용 분야에서 활용될 수 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C0EvrTWA3me3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBRbh4NiQDil"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "\n",
        "class ChromaParallel(Chroma):\n",
        "\n",
        "    async def afrom_documents(documents, embedding, num_workers=2):\n",
        "        db = Chroma(embedding_function=embedding)\n",
        "        # create list of num_workers empty lists\n",
        "        doc_groups = [[] for _ in range(num_workers)]\n",
        "\n",
        "        for i in range(len(documents)):\n",
        "            doc_groups[i % num_workers].append(documents[i])\n",
        "\n",
        "        tasks = [db.aadd_documents(group) for group in doc_groups]\n",
        "        await asyncio.gather(*tasks)\n",
        "        return db"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChromaParallel 클래스의 afrom_documents 메서드는 비동기 병렬 처리를 통해 문서 임베딩을 동시에 처리하여 속도를 향상시킵니다. num_workers를 통해 작업자 수를 설정할 수 있어, 문서의 양이나 시스템 리소스에 맞춰 최적의 성능을 낼 수 있습니다."
      ],
      "metadata": {
        "id": "_SN08hSz39uM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coPCIWV1QDil",
        "outputId": "12236957-501e-4b3f-8387-f85c3c4a0566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='correct classifications of<br>the type (nb→b) over the total number of classificationsthat<br>resulted in a bug outcome. Put another way, if the change<br>classifier predicts that athat a change is buggy, what fraction of<br>these changes really contains a bug?</p><figure><img'\n",
            "Wall time: 22.16 sec\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import time\n",
        "\n",
        "now = time.time()\n",
        "\n",
        "# 3. Embed & indexing\n",
        "loop = asyncio.get_event_loop()\n",
        "semantic_vectorstore = await ChromaParallel.afrom_documents(\n",
        "    documents=semantic_splits,\n",
        "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
        "    num_workers=3,\n",
        ")\n",
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# 4. retrive\n",
        "result_docs = semantic_retriever.invoke(\"What is Bug Classification?\")\n",
        "print(result_docs[1])\n",
        "print(f\"Wall time: {time.time() - now:.2f} sec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드는 3개의 작업자를 활용한 비동기 병렬 임베딩 및 검색 작업을 수행합니다. ChromaParallel 클래스와 afrom_documents 메서드를 통해 임베딩이 병렬로 수행되며, 검색 작업도 빠르게 수행되어 효율적인 RAG 파이프라인 구축에 도움이 됩니다. Wall time 출력은 전체 코드 실행에 걸린 시간을 나타내며, 병렬 처리를 통해 시간을 단축할 수 있음을 보여줍니다."
      ],
      "metadata": {
        "id": "cOPgKfpQ4Hba"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NdadABZQDil",
        "outputId": "a686f551-53ad-401a-f454-d20b46ab5323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bug classification is a process of categorizing software bugs based on their characteristics, such as the type of error, the component affected, the severity of the issue, or the root cause. This process helps developers and testers to prioritize and manage bugs more effectively.\n",
            "\n",
            "Bug classification typically works by following these steps:\n",
            "\n",
            "1. **Identification**: The first step is to identify the bug. This is usually done through testing or user reports.\n",
            "\n",
            "2. **Reporting**: The bug is then reported, usually through a bug tracking system, where it is assigned a unique identifier.\n",
            "\n",
            "3. **Analysis**: The bug is analyzed to determine its nature, severity, and impact on the software. This analysis may involve reproducing the bug, investigating its cause, and assessing how it affects the software's functionality.\n",
            "\n",
            "4. **Categorization**: Based on the analysis, the bug is classified into a specific category. This categorization can be done using predefined categories or custom categories specific to the project.\n",
            "\n",
            "5. **Prioritization**: Once the bug is classified, it is prioritized based on its severity, impact, and other factors. This prioritization helps developers to focus on the most critical bugs first.\n",
            "\n",
            "6. **Resolution**: The bug is then fixed by the development team, and once it is resolved, it is marked as \"closed\" or \"fixed\" in the bug tracking system.\n",
            "\n",
            "7. **Verification**: The fixed bug is verified to ensure that it has been resolved correctly and does not introduce new issues.\n",
            "\n",
            "By classifying bugs in this way, development teams can better manage their bug backlog, allocate resources more efficiently, and ensure that the most critical issues are addressed first.\n"
          ]
        }
      ],
      "source": [
        "# Finally query using RAG\n",
        "query = \"What is bug classification? How it works?\"\n",
        "result_docs = semantic_retriever.invoke(query)\n",
        "\n",
        "gc_result = chain.invoke({\"history\": history, \"context\": result_docs, \"input\": query})\n",
        "print(gc_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드에서는 RAG 파이프라인을 통해 최종 질문에 대해 맥락 기반의 답변을 생성합니다. 검색된 문서와 대화 히스토리를 기반으로 하여, 사용자 질문에 맞는 정확하고 연관성 있는 답변을 제공합니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W7cvtJC64Q8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXEhCcigQDil",
        "outputId": "810e3ac0-7087-4f47-a3b2-574b3da06783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifying bugs is beneficial for several reasons:\n",
            "\n",
            "1. **Prioritization**: By categorizing bugs based on their severity, impact, and other factors, development teams can prioritize their work and focus on the most critical issues first. This helps to ensure that the most pressing problems are addressed promptly, improving the overall quality of the software.\n",
            "\n",
            "2. **Efficiency**: Classifying bugs allows teams to manage their workload more effectively. They can assign bugs to specific team members based on their expertise or availability, streamlining the bug fixing process.\n",
            "\n",
            "3. **Transparency**: A well-defined bug classification system provides transparency into the bug tracking process. This enables stakeholders, such as project managers, to understand the current state of the software and make informed decisions about resource allocation and project timelines.\n",
            "\n",
            "4. **Improved Communication**: Clear bug classification categories facilitate better communication within the development team and between the team and other stakeholders. Everyone involved in the project can understand the nature and severity of the bugs, making it easier to coordinate efforts and track progress.\n",
            "\n",
            "5. **Quality Assurance**: Bug classification helps in identifying patterns and trends in the types of bugs that occur in the software. This information can be used to improve the development process, prevent similar bugs from occurring in the future, and enhance the overall quality of the software.\n",
            "\n",
            "6. **Customer Satisfaction**: By effectively managing and resolving bugs, development teams can improve the user experience and increase customer satisfaction. This can lead to better product adoption, positive reviews, and increased customer loyalty.\n",
            "\n",
            "In summary, bug classification is a crucial part of the software development process that helps teams to manage their workload, improve communication, and ensure that the software is of high quality and meets user expectations.\n"
          ]
        }
      ],
      "source": [
        "history = [HumanMessage(query), AIMessage(gc_result)]\n",
        "\n",
        "query = \"Why it is good?\"\n",
        "result_docs = semantic_retriever.invoke(query)\n",
        "\n",
        "gc_result = chain.invoke({\"history\": history, \"context\": result_docs, \"input\": query})\n",
        "print(gc_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드에서는 대화 히스토리를 유지하면서 새로운 질문에 대한 응답을 생성합니다. 검색된 문서와 이전 대화 내용을 기반으로 모델이 일관성 있고 연관성 있는 답변을 제공하여, 사용자와의 대화를 이어갈 수 있습니다."
      ],
      "metadata": {
        "id": "BIAZ8_Bo4hGH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx7YOU5pQDim"
      },
      "source": [
        "### For an in-depth look at the different types of RAG, please refer to the files '09. Smart RAG' and '10. Tool_RAG'.\n",
        "\n",
        "- [09. Smart RAG.ipynb](https://github.com/UpstageAI/cookbook/blob/main/cookbooks/upstage/Solar-Full-Stack-LLM-101/09_Smart_RAG.ipynb)\n",
        "- [10. Tool_RAG.ipynb](https://github.com/UpstageAI/cookbook/blob/main/cookbooks/upstage/Solar-Full-Stack-LLM-101/10_tool_RAG.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Pj7q1FQDim"
      },
      "source": [
        "## [Session 5] Gradio\n",
        "\n",
        "<b> Comprehensive RAG System for PDFs </b> : Use Gradio and RAG techniques to process PDF documents and generate real-time, interactive responses."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio와 **RAG(검색 보강 생성)**을 결합하여 PDF 문서를 실시간으로 처리하고 대화형 응답을 생성하는 시스템을 구축할 수 있습니다. Gradio는 사용자와의 인터랙티브 웹 인터페이스를 쉽게 만들어 주며, RAG는 질문에 맞는 정보를 PDF에서 추출하여 답변을 제공하는 데 사용됩니다.\n",
        "\n",
        "주요 구성 요소\n",
        "Gradio 인터페이스:\n",
        "\n",
        "Gradio를 사용해 사용자가 PDF 파일을 업로드하고 질문을 입력할 수 있는 웹 인터페이스를 생성합니다.\n",
        "실시간 대화형 응답을 제공하여 사용자 경험을 개선합니다.\n",
        "RAG 시스템:\n",
        "\n",
        "PDF에서 텍스트를 추출하고, 이를 임베딩하여 벡터 데이터베이스에 저장합니다.\n",
        "사용자가 질문을 입력하면 관련 있는 정보를 데이터베이스에서 검색하고, 이를 기반으로 응답을 생성합니다.\n",
        "구현 단계\n",
        "PDF 문서 로드 및 텍스트 추출:\n",
        "\n",
        "PyMuPDF 또는 pdfplumber와 같은 PDF 라이브러리를 사용해 PDF에서 텍스트를 추출합니다.\n",
        "추출된 텍스트를 의미 단위로 나누고, 임베딩을 생성하여 벡터 데이터베이스에 저장합니다.\n",
        "Gradio 웹 인터페이스 구축:\n",
        "\n",
        "Gradio에서 PDF 업로드 및 질문 입력 필드를 만들고, 응답을 실시간으로 보여주는 UI를 구성합니다.\n",
        "RAG 파이프라인 구축:\n",
        "\n",
        "Chroma와 같은 벡터 데이터베이스를 사용해 문서에서 유사한 텍스트 청크를 검색하고, 검색된 정보로 응답을 생성합니다.\n",
        "질문에 맞는 청크를 찾아 모델이 적절한 답변을 제공할 수 있도록 합니다.\n",
        "코드 예시"
      ],
      "metadata": {
        "id": "k_7FfTd15Cw-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ0cFLubQDim",
        "outputId": "a4e902d3-8b87-4eb3-da7e-0cf16e42d56b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU gradio python-dotenv langchain-upstage python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hefLvQkoQDim"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "from langchain_upstage import (\n",
        "    ChatUpstage,\n",
        "    UpstageEmbeddings,\n",
        "    UpstageLayoutAnalysisLoader,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain_text_splitters import (\n",
        "    Language,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "llm = ChatUpstage(streaming=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드는 Gradio와 LangChain, Upstage 및 Chroma를 결합하여 PDF 문서에서 질문에 대한 응답을 생성하는 대화형 RAG(검색 보강 생성) 시스템을 구축하는 설정을 하고 있습니다."
      ],
      "metadata": {
        "id": "MdWWZBuj5Ksd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAYDE-vKQDim"
      },
      "outputs": [],
      "source": [
        "# More general chat\n",
        "chat_with_history_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{message}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드는 대화 히스토리를 반영하는 채팅 프롬프트를 생성하기 위한 설정입니다. ChatPromptTemplate.from_messages() 메서드를 통해 대화의 맥락을 유지하며 답변을 생성할 수 있도록 history를 포함한 프롬프트 템플릿을 만듭니다."
      ],
      "metadata": {
        "id": "yPD9Q6Sv5RL3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frL6vGBKQDin"
      },
      "outputs": [],
      "source": [
        "chain = chat_with_history_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_97FabRZQDin"
      },
      "outputs": [],
      "source": [
        "def chat(message, history):\n",
        "    history_langchain_format = []\n",
        "    for human, ai in history:\n",
        "        history_langchain_format.append(HumanMessage(content=human))\n",
        "        history_langchain_format.append(AIMessage(content=ai))\n",
        "\n",
        "    return chain.invoke({\"message\": message, \"history\": history_langchain_format})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수는 대화의 연속성을 유지하면서 사용자 메시지에 응답하는 기능을 수행합니다. history를 LangChain의 형식에 맞게 변환하고, 새로운 message와 함께 전달하여, 문맥에 맞는 응답을 생성할 수 있습니다."
      ],
      "metadata": {
        "id": "FB2w0lQH5bhD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-ux6Ps1QDin"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.ChatInterface(\n",
        "        chat,\n",
        "        examples=[\n",
        "            \"How to eat healthy?\",\n",
        "            \"Best Places in Korea\",\n",
        "            \"How to make a chatbot?\",\n",
        "        ],\n",
        "        title=\"Solar Chatbot\",\n",
        "        description=\"Upstage Solar Chatbot\",\n",
        "    )\n",
        "    chatbot.chatbot.height = 300"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드는 Gradio와 LangChain 기반의 chat 함수를 활용하여 대화형 챗봇 인터페이스를 생성합니다. gr.Blocks() 내에서 gr.ChatInterface를 사용하여 Gradio의 대화형 인터페이스를 설정하며, Solar Chatbot이라는 이름으로 사용자와 상호작용할 수 있습니다."
      ],
      "metadata": {
        "id": "NwbfXIb75hZX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XjfghxDQDin",
        "outputId": "3227c915-0b88-4f7c-eab6-be0d6cf6bab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHh0nT3bQDin"
      },
      "source": [
        "### Gradio_ChatPDF\n",
        "\n",
        "- <b> Comprehensive RAG System for PDFs </b> : Use Gradio and RAG techniques to process PDF documents and generate real-time, interactive responses."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprehensive RAG System for PDFs는 Gradio와 RAG(검색 보강 생성) 기술을 결합하여, PDF 문서에서 실시간 대화형 응답을 생성하는 시스템입니다. 이 시스템은 PDF에서 텍스트를 추출하고, 질문에 따라 관련 정보를 검색하여 대화형으로 답변을 제공하도록 설계됩니다."
      ],
      "metadata": {
        "id": "ZIPGtzjP5pKa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOaS_YHZQDin"
      },
      "outputs": [],
      "source": [
        "def chat(message, history, retriever):\n",
        "    result_docs = \"\"\n",
        "    if retriever:\n",
        "        result_docs = retriever.invoke(message)\n",
        "\n",
        "    history_langchain_format = []\n",
        "    for human, ai in history:\n",
        "        history_langchain_format.append(HumanMessage(content=human))\n",
        "        history_langchain_format.append(AIMessage(content=ai))\n",
        "\n",
        "    generator = chain.stream(\n",
        "        {\n",
        "            \"context\": result_docs,\n",
        "            \"message\": message,\n",
        "            \"history\": history_langchain_format,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    assistant = \"\"\n",
        "    for gen in generator:\n",
        "        assistant += gen\n",
        "        yield assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수는 Gradio와 LangChain을 사용하여 실시간으로 응답을 생성하는 대화형 챗봇 시스템입니다. PDF 문서에서 정보를 검색하여 질문에 대한 대답을 생성하며, 답변을 스트리밍 방식으로 실시간 출력합니다."
      ],
      "metadata": {
        "id": "CrR6winC5vTH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjlkzxNpQDin"
      },
      "outputs": [],
      "source": [
        "def file_upload(file):\n",
        "    layzer = UpstageLayoutAnalysisLoader(file, output_type=\"html\", use_ocr=False)\n",
        "    docs = layzer.load()\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "    print(len(splits))\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=splits,\n",
        "        embedding=UpstageEmbeddings(\n",
        "            model=\"solar-embedding-1-large\", embed_batch_size=100\n",
        "        ),\n",
        "    )\n",
        "    retriever = vectorstore.as_retriever()\n",
        "\n",
        "    return file, retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수는 PDF 파일을 업로드하여 텍스트를 추출하고 분할한 뒤, 벡터 스토어에 저장하고 검색기를 생성합니다. RAG 시스템의 초기 설정 단계로, 업로드된 파일에서 관련 정보를 추출하고, 질문 응답을 위한 검색기를 반환하도록 설계되었습니다."
      ],
      "metadata": {
        "id": "xttsNbrQ51wc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0241a3C0QDin"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Solar Chatbot\")\n",
        "    gr.Markdown(\n",
        "        \"Upstage Solar Chatbot\",\n",
        "    )\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            file = gr.File()\n",
        "            retreiver = gr.State()\n",
        "        with gr.Column():\n",
        "            chatbot = gr.ChatInterface(\n",
        "                chat,\n",
        "                examples=[\n",
        "                    [\"How to eat healthy?\"],\n",
        "                    [\"Best Places in Korea\"],\n",
        "                    [\"How to make a chatbot?\"],\n",
        "                ],\n",
        "                additional_inputs=retreiver,\n",
        "            )\n",
        "    chatbot.chatbot.height = 300\n",
        "    file.upload(file_upload, file, [file, retreiver])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드는 Gradio를 사용하여 PDF 문서를 업로드하고 질문에 응답하는 Solar Chatbot 인터페이스를 만드는 설정입니다. 사용자가 PDF 파일을 업로드하면, 시스템이 파일을 처리하여 검색기를 생성하고, 사용자가 입력하는 질문에 대한 응답을 실시간으로 제공합니다."
      ],
      "metadata": {
        "id": "0zkna4oY59MJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLCZ-odPQDio",
        "outputId": "feb54727-dc10-4a89-d3c1-708e5b400f99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7862\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jF9SgpIQDio"
      },
      "source": [
        "## 🚀 Building Your Own AI-Powered Chatbot! 🤖\n",
        "\n",
        "\n",
        "Congratulations on completing the course on building chatbots using Language Models (LLMs), Layout Analysis (LA), custom tools, and Groundedness Checks (GC)! Now, showcase your brilliant ideas by participating in a hackathon and leveraging the Solar API! 🚀"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import re\n",
        "from langchain.chat_models import ChatOpenAI  # 대체 LLM 모델\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # 대체 임베딩 모델\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.output_parsers import StrOutputParser\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "\n",
        "\n",
        "\n",
        "# 엑셀 파일에서 용어와 정의를 로드하는 함수\n",
        "def load_terms_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    print(\"Excel loaded data:\", df.head())\n",
        "    term_dict = dict(zip(df['term'], df['definition']))\n",
        "    return term_dict\n",
        "\n",
        "# 엑셀 파일에서 용어와 정의 로드\n",
        "terms = load_terms_from_excel('/content/excel.xlsx')  # 엑셀 파일 경로 설정\n",
        "print(\"Loaded terms dictionary:\", terms)  # 로드된 용어 출력\n",
        "\n",
        "# LLM 설정 (ChatOpenAI로 대체)\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", streaming=True)\n",
        "\n",
        "# 대화 이력을 포함하는 프롬프트 템플릿 생성\n",
        "chat_with_history_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 유용한 어시스턴트입니다.\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{message}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 검색 함수 정의\n",
        "def retrieve_documents(query):\n",
        "    # 입력된 쿼리에서 엑셀 파일의 용어와 정의를 검색\n",
        "    results = {term: definition for term, definition in terms.items() if term.lower() in query.lower()}\n",
        "    return results\n",
        "\n",
        "# 프롬프트 템플릿과 LLM, 출력 파서를 연결하여 체인 생성\n",
        "chain = chat_with_history_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 대화 함수 정의\n",
        "def chat(message, history):\n",
        "    # 엑셀 파일에서 용어 정의 검색\n",
        "    retrieved_docs = retrieve_documents(message)\n",
        "\n",
        "    # 엑셀 파일에서 정의가 검색된 경우 이를 반환\n",
        "    if retrieved_docs:\n",
        "        response = \"인식된 용어와 정의:\\n\\n\"\n",
        "        for term, definition in retrieved_docs.items():\n",
        "            response += f\"{term}: {definition}\\n\\n\"\n",
        "        return response  # 엑셀 파일의 정의를 우선적으로 반환\n",
        "\n",
        "    # 엑셀 파일의 정의가 없는 경우 LLM을 통해 답변 생성\n",
        "    history_langchain_format = []\n",
        "    for human, ai in history:\n",
        "        history_langchain_format.append(HumanMessage(content=human))\n",
        "        history_langchain_format.append(AIMessage(content=ai))\n",
        "\n",
        "    # 체인을 호출하여 응답 생성\n",
        "    return chain.invoke({\"message\": message, \"history\": history_langchain_format})\n",
        "\n",
        "# Gradio 인터페이스 설정\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.ChatInterface(\n",
        "        chat,\n",
        "        examples=[\n",
        "            \"Explain AI\",\n",
        "            \"What is Machine Learning?\",\n",
        "            \"Define Natural Language Processing\"\n",
        "        ],\n",
        "        title=\"Solar Chatbot\",\n",
        "        description=\"Upstage Solar Chatbot\"\n",
        "    )\n",
        "    chatbot.chatbot.height = 300\n",
        "\n",
        "# 프로그램 시작 지점\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "id": "WDuhJHYZBM9S",
        "outputId": "b5df1616-5d66-4a93-ef42-c8a642569ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.7)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.15)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.6->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.6->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain_community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'StrOutputParser' from 'langchain.output_parsers' (/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2bd0ed4711fb>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m  \u001b[0;31m# 대체 임베딩 모델\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatPromptTemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMessagesPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parsers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStrOutputParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAIMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHumanMessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'StrOutputParser' from 'langchain.output_parsers' (/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import re\n",
        "from langchain.chat_models import ChatOpenAI  # 대체 LLM 모델\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # 대체 임베딩 모델\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "\n",
        "# 엑셀 파일에서 용어와 정의를 로드하는 함수\n",
        "def load_terms_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    print(\"Excel loaded data:\", df.head())\n",
        "    term_dict = dict(zip(df['term'], df['definition']))\n",
        "    return term_dict\n",
        "\n",
        "# 엑셀 파일에서 용어와 정의 로드\n",
        "terms = load_terms_from_excel('/content/excel.xlsx')  # 엑셀 파일 경로 설정\n",
        "print(\"Loaded terms dictionary:\", terms)  # 로드된 용어 출력\n",
        "\n",
        "# LLM 설정 (ChatOpenAI로 대체)\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", streaming=True)\n",
        "\n",
        "# 대화 이력을 포함하는 프롬프트 템플릿 생성\n",
        "chat_with_history_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 유용한 어시스턴트입니다.\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{message}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 검색 함수 정의\n",
        "def retrieve_documents(query):\n",
        "    # 입력된 쿼리에서 엑셀 파일의 용어와 정의를 검색\n",
        "    results = {term: definition for term, definition in terms.items() if term.lower() in query.lower()}\n",
        "    return results\n",
        "\n",
        "# 대화 함수 정의\n",
        "def chat(message, history):\n",
        "    # 엑셀 파일에서 용어 정의 검색\n",
        "    retrieved_docs = retrieve_documents(message)\n",
        "\n",
        "    # 엑셀 파일에서 정의가 검색된 경우 이를 반환\n",
        "    if retrieved_docs:\n",
        "        response = \"인식된 용어와 정의:\\n\\n\"\n",
        "        for term, definition in retrieved_docs.items():\n",
        "            response += f\"{term}: {definition}\\n\\n\"\n",
        "        return response  # 엑셀 파일의 정의를 우선적으로 반환\n",
        "\n",
        "    # 엑셀 파일의 정의가 없는 경우 LLM을 통해 답변 생성\n",
        "    history_langchain_format = []\n",
        "    for human, ai in history:\n",
        "        history_langchain_format.append(HumanMessage(content=human))\n",
        "        history_langchain_format.append(AIMessage(content=ai))\n",
        "\n",
        "    # LLM 호출하여 응답 생성\n",
        "    response = llm.generate({\"message\": message, \"history\": history_langchain_format})\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "# Gradio 인터페이스 설정\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.ChatInterface(\n",
        "        chat,\n",
        "        examples=[\n",
        "            \"Explain AI\",\n",
        "            \"What is Machine Learning?\",\n",
        "            \"Define Natural Language Processing\"\n",
        "        ],\n",
        "        title=\"Solar Chatbot\",\n",
        "        description=\"Upstage Solar Chatbot\"\n",
        "    )\n",
        "    chatbot.chatbot.height = 300\n",
        "\n",
        "# 프로그램 시작 지점\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "id": "_IyEvtW8Crkw",
        "outputId": "50c4d483-feec-4d37-83d8-7425c0c13dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excel loaded data:   term definition\n",
            "0   사과  빨갛고 둥근 과일\n",
            "1  바나나   노랗고 긴 과일\n",
            "2   포도    보라색의 과일\n",
            "3    배  노랗고 둥근 과일\n",
            "Loaded terms dictionary: {'사과': '빨갛고 둥근 과일', '바나나': '노랗고 긴 과일', '포도': '보라색의 과일', '배': '노랗고 둥근 과일'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:229: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7081f117a6e930fe28.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7081f117a6e930fe28.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n_888MxLBs3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "39ce5e4c64934c7ba2df82ad1e4f1498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_493aa6d96f7e4ce0bb91666f28d5001d",
              "IPY_MODEL_7be12a97a5af4ad98ba581382d933243",
              "IPY_MODEL_b1d507020abc4873bfdc14dcda8193c2"
            ],
            "layout": "IPY_MODEL_ad7416be07a1466ca82f9f4a93b4c21e"
          }
        },
        "493aa6d96f7e4ce0bb91666f28d5001d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c9dd4908c794969bcc9e4c39c0e47f7",
            "placeholder": "​",
            "style": "IPY_MODEL_bfc5d2260e484de2be4110d292ddbce0",
            "value": ""
          }
        },
        "7be12a97a5af4ad98ba581382d933243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b537937fc314b8d8eb319ec2244cf5b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c1185dc55ef4f85a4bfe74a9755bdac",
            "value": 0
          }
        },
        "b1d507020abc4873bfdc14dcda8193c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba069a59de7e4e65947bcc81ca6ba947",
            "placeholder": "​",
            "style": "IPY_MODEL_0e20f1659ced4708902011d66748ea7a",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "ad7416be07a1466ca82f9f4a93b4c21e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c9dd4908c794969bcc9e4c39c0e47f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfc5d2260e484de2be4110d292ddbce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b537937fc314b8d8eb319ec2244cf5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2c1185dc55ef4f85a4bfe74a9755bdac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba069a59de7e4e65947bcc81ca6ba947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e20f1659ced4708902011d66748ea7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c3ffd90820b4fbcb4c33b3ba5110596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1afb1d971304e95a68811c4762698b0",
              "IPY_MODEL_fb4d9944561f4678aeeaa7a24d32826c",
              "IPY_MODEL_0f8c225633564210bd7b8dd9c5035049"
            ],
            "layout": "IPY_MODEL_9bbe32ab4e434b64879bc858c2f58dab"
          }
        },
        "c1afb1d971304e95a68811c4762698b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a708818f5363498da9afa8300c632026",
            "placeholder": "​",
            "style": "IPY_MODEL_da0f3c009ffe4ed1a09a798152da622d",
            "value": "config.json: 100%"
          }
        },
        "fb4d9944561f4678aeeaa7a24d32826c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0d30de80b354150891d4370725d8289",
            "max": 545,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f216c978eb2946bcb6de1497c9b5ee66",
            "value": 545
          }
        },
        "0f8c225633564210bd7b8dd9c5035049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8db6ccbb17ed40cab842256122c98bed",
            "placeholder": "​",
            "style": "IPY_MODEL_995e89b93ea8467084ee9c9616fb370e",
            "value": " 545/545 [00:00&lt;00:00, 27.1kB/s]"
          }
        },
        "9bbe32ab4e434b64879bc858c2f58dab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a708818f5363498da9afa8300c632026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da0f3c009ffe4ed1a09a798152da622d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0d30de80b354150891d4370725d8289": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f216c978eb2946bcb6de1497c9b5ee66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8db6ccbb17ed40cab842256122c98bed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "995e89b93ea8467084ee9c9616fb370e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8496a8f7fd746a58ab0ab38a614e70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a174ca6d295648e3a3fa0666aa5483a6",
              "IPY_MODEL_2ec1a35c4ca84727a21d8d2dd675b378",
              "IPY_MODEL_74afa7401cbc41b3805a3fd74efef7a4"
            ],
            "layout": "IPY_MODEL_33f483ed5ac446439d30adc2ac9e04c3"
          }
        },
        "a174ca6d295648e3a3fa0666aa5483a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b68d226b84ec43e0b0751129d80d7eaf",
            "placeholder": "​",
            "style": "IPY_MODEL_fd32b39270934512be7f742b269ab76c",
            "value": "model.safetensors: 100%"
          }
        },
        "2ec1a35c4ca84727a21d8d2dd675b378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1a6aacc9cb5433ea39b5dface27a11f",
            "max": 272514464,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4ef6a5028734105ad354669054a4722",
            "value": 272514464
          }
        },
        "74afa7401cbc41b3805a3fd74efef7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_814c8d9c524d4b52a7c30d5b101f8013",
            "placeholder": "​",
            "style": "IPY_MODEL_e13873faabb74a2d990c03ca171c7fa8",
            "value": " 273M/273M [00:03&lt;00:00, 86.6MB/s]"
          }
        },
        "33f483ed5ac446439d30adc2ac9e04c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b68d226b84ec43e0b0751129d80d7eaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd32b39270934512be7f742b269ab76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1a6aacc9cb5433ea39b5dface27a11f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4ef6a5028734105ad354669054a4722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "814c8d9c524d4b52a7c30d5b101f8013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e13873faabb74a2d990c03ca171c7fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da060a1b2f554181921b6f21445ef667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09f9fd2d872d4d1ba3982187073a59b7",
              "IPY_MODEL_b7bbb64b314e455981cbf396a477122f",
              "IPY_MODEL_e350da9ea90a4605b14afc66e43e8d66"
            ],
            "layout": "IPY_MODEL_fc22a313f47b490eae4c52c56a79b5f4"
          }
        },
        "09f9fd2d872d4d1ba3982187073a59b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_217cdf372f304f7986824dc289adfe51",
            "placeholder": "​",
            "style": "IPY_MODEL_a45ef0c2a78d482ba0ee7352b4308d72",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b7bbb64b314e455981cbf396a477122f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec3ffe596ea642b59bdce97facf82afc",
            "max": 375,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d991a9a7f3214402ab45b29954e4031b",
            "value": 375
          }
        },
        "e350da9ea90a4605b14afc66e43e8d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d775ae7dd024dc0b457e0c7971e9f07",
            "placeholder": "​",
            "style": "IPY_MODEL_97b2714b768b4928997402a786fc2d24",
            "value": " 375/375 [00:00&lt;00:00, 9.13kB/s]"
          }
        },
        "fc22a313f47b490eae4c52c56a79b5f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "217cdf372f304f7986824dc289adfe51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a45ef0c2a78d482ba0ee7352b4308d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec3ffe596ea642b59bdce97facf82afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d991a9a7f3214402ab45b29954e4031b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d775ae7dd024dc0b457e0c7971e9f07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97b2714b768b4928997402a786fc2d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b086a0e8c22d4a27b7225f4fa617e1c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2172d2509ede41e1a871bb66b0eb4ecd",
              "IPY_MODEL_89096d8969314395b18a928e28364047",
              "IPY_MODEL_1dd4a9b196594c4c9803ed7e47fab580"
            ],
            "layout": "IPY_MODEL_04ae0fdf6dd84219b7f8440d1abd5c72"
          }
        },
        "2172d2509ede41e1a871bb66b0eb4ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23e144bde5bc427a8529062ccd1c60e5",
            "placeholder": "​",
            "style": "IPY_MODEL_dcfa07e2c99e4598a3aecbb32632b859",
            "value": "vocab.txt: 100%"
          }
        },
        "89096d8969314395b18a928e28364047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cd27ff43fab48fdb7478b2791524cf2",
            "max": 248477,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ecbbda04ab80494aa09c79d3eef19bc3",
            "value": 248477
          }
        },
        "1dd4a9b196594c4c9803ed7e47fab580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e277eb77d12348fb892f94e576e9de65",
            "placeholder": "​",
            "style": "IPY_MODEL_43b3e296e56b4245af5fc6a013189e5b",
            "value": " 248k/248k [00:00&lt;00:00, 1.25MB/s]"
          }
        },
        "04ae0fdf6dd84219b7f8440d1abd5c72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e144bde5bc427a8529062ccd1c60e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcfa07e2c99e4598a3aecbb32632b859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cd27ff43fab48fdb7478b2791524cf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecbbda04ab80494aa09c79d3eef19bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e277eb77d12348fb892f94e576e9de65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43b3e296e56b4245af5fc6a013189e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c0f4af089054804a4bf4f71e9108895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa4baedd966346b0a840b237f8fcb6ba",
              "IPY_MODEL_15a9632bba6a42d68768c1c7932d5fd1",
              "IPY_MODEL_37d479a484e64154b921941bcbd8877a"
            ],
            "layout": "IPY_MODEL_dad776c665b64ef4a2289bb01fe2ae01"
          }
        },
        "fa4baedd966346b0a840b237f8fcb6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_824d8364c4294db5aba39ef680e73a8d",
            "placeholder": "​",
            "style": "IPY_MODEL_888f0347f0aa488b951d53b27c6edecf",
            "value": "tokenizer.json: 100%"
          }
        },
        "15a9632bba6a42d68768c1c7932d5fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b163ecf4cfc449a4b10743e08bce0736",
            "max": 751504,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6db847034f2c455e99436e2970f6813f",
            "value": 751504
          }
        },
        "37d479a484e64154b921941bcbd8877a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_739c6d38b015434d9f646c16ebf96570",
            "placeholder": "​",
            "style": "IPY_MODEL_833655151f14441a9567760e1d883ea1",
            "value": " 752k/752k [00:00&lt;00:00, 7.75MB/s]"
          }
        },
        "dad776c665b64ef4a2289bb01fe2ae01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "824d8364c4294db5aba39ef680e73a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "888f0347f0aa488b951d53b27c6edecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b163ecf4cfc449a4b10743e08bce0736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6db847034f2c455e99436e2970f6813f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "739c6d38b015434d9f646c16ebf96570": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "833655151f14441a9567760e1d883ea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa64a131b11840f6a4e5161187cdff65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a63cb9a42b8341e8a58737c42aa575be",
              "IPY_MODEL_2db720073c8643c99ff382d28e8a2eda",
              "IPY_MODEL_04cc8b20f54149db99a2391109bb5e01"
            ],
            "layout": "IPY_MODEL_c0c35a88ade0430582903b32ae276633"
          }
        },
        "a63cb9a42b8341e8a58737c42aa575be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_501c347c2fb94d988aacb318d40d0141",
            "placeholder": "​",
            "style": "IPY_MODEL_ed356d80b19445a2b7ef30e8e2ecf938",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "2db720073c8643c99ff382d28e8a2eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_840a339c1a5d432fa9478aa7930cad24",
            "max": 173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21caaf4e1b7c48ea9d9137f26deb5baf",
            "value": 173
          }
        },
        "04cc8b20f54149db99a2391109bb5e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba5a1f3aa6c548edaa21e53469716151",
            "placeholder": "​",
            "style": "IPY_MODEL_d8d530d5178c45a0bb6593c19d6ce421",
            "value": " 173/173 [00:00&lt;00:00, 4.10kB/s]"
          }
        },
        "c0c35a88ade0430582903b32ae276633": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "501c347c2fb94d988aacb318d40d0141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed356d80b19445a2b7ef30e8e2ecf938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "840a339c1a5d432fa9478aa7930cad24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21caaf4e1b7c48ea9d9137f26deb5baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba5a1f3aa6c548edaa21e53469716151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8d530d5178c45a0bb6593c19d6ce421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f25efaeb5c5b45b2a36633b0128d3b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a146b57d8834ce19153ef83efc50b0c",
              "IPY_MODEL_2e6d1777ea4a47d19ad68c76e3e5ebe1",
              "IPY_MODEL_5d7e4fd59abe48729c4d5b0431c823f9"
            ],
            "layout": "IPY_MODEL_f4ac7d27b215458087b5f99f8c839cc0"
          }
        },
        "4a146b57d8834ce19153ef83efc50b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4c945b3ff644468b3f5aa9f983c7281",
            "placeholder": "​",
            "style": "IPY_MODEL_631ae0ccb8a94ceb89a8b66b6540f873",
            "value": "tokenizer.json: 100%"
          }
        },
        "2e6d1777ea4a47d19ad68c76e3e5ebe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4f72e0f65574313a2c6b146e2d73376",
            "max": 3314725,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_657b71996e044b6a8812447568835478",
            "value": 3314725
          }
        },
        "5d7e4fd59abe48729c4d5b0431c823f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f73fd1da56a04d0e95a73bf7b34d9f27",
            "placeholder": "​",
            "style": "IPY_MODEL_95e94bef499b4e1ab34eaf1c57a8c921",
            "value": " 3.31M/3.31M [00:00&lt;00:00, 10.0MB/s]"
          }
        },
        "f4ac7d27b215458087b5f99f8c839cc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c945b3ff644468b3f5aa9f983c7281": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "631ae0ccb8a94ceb89a8b66b6540f873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4f72e0f65574313a2c6b146e2d73376": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "657b71996e044b6a8812447568835478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f73fd1da56a04d0e95a73bf7b34d9f27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95e94bef499b4e1ab34eaf1c57a8c921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}